name: Run MLE-Bench Agent

on:
  workflow_dispatch:
    inputs:
      competition_set:
        description: 'Competition set to run'
        required: true
        type: choice
        options:
          - custom-set.txt
          - low.txt (Lite - 21 competitions)
          - medium.txt (38 competitions)
          - high.txt (14 competitions)
          - dev.txt (6 competitions)
        default: 'custom-set.txt'

      custom_competitions:
        description: 'Custom competitions (comma-separated, e.g. "aerial-cactus-identification,cifar-10-classification", overrides competition_set if provided)'
        required: false
        type: string

      dry_run:
        description: 'Dry run mode (validate without executing)'
        required: false
        type: boolean
        default: false

      target_gpu:
        description: 'Target specific GPU machine (leave empty for any available)'
        required: false
        type: choice
        options:
          - any
          - gpu-1
          - gpu-2
          - gpu-3
          - gpu-4
          - gpu-5
          - gpu-6
          - gpu-7
          - gpu-8
        default: 'any'

      rebuild_image:
        description: 'Rebuild Docker image from Dockerfile'
        required: false
        type: boolean
        default: false

jobs:
  run-agent:
    runs-on:
      - self-hosted
      - gpu
      - ${{ github.event.inputs.target_gpu != 'any' && github.event.inputs.target_gpu || 'gpu' }}

    timeout-minutes: 1440

    steps:
      - name: Show run configuration
        run: |
          echo "=========================================="
          echo "MLE-Bench Run Configuration"
          echo "=========================================="
          echo "Branch: ${{ github.ref_name }}"
          echo "Competition set: ${{ github.event.inputs.competition_set }}"
          echo "Dry run: ${{ github.event.inputs.dry_run }}"
          echo "Rebuild image: ${{ github.event.inputs.rebuild_image }}"
          echo "Target GPU: ${{ github.event.inputs.target_gpu }}"
          echo "Runner: $(hostname)"
          echo "Run ID: ${{ github.run_id }}"
          echo "Run Number: ${{ github.run_number }}"
          echo "=========================================="

      - name: Clean workspace
        run: |
          echo "Cleaning workspace..."
          cd $GITHUB_WORKSPACE || true
          find . -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null || true
          find . -type f -name "*.pyc" -delete 2>/dev/null || true
          find . -type f -name "*.pyo" -delete 2>/dev/null || true
          git clean -fdx || true
          git reset --hard HEAD || true
          echo "✅ Workspace cleaned (including Python bytecode)"

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          clean: true
          lfs: true

      - name: Setup competition set
        run: |
          cd mle-bench/experiments/splits

          if [ -n "${{ github.event.inputs.custom_competitions }}" ]; then
            echo "Using custom competitions list (comma-separated):"
            echo "${{ github.event.inputs.custom_competitions }}"

            # Convert comma-separated to newline-separated
            echo "${{ github.event.inputs.custom_competitions }}" | tr ',' '\n' | sed 's/^[[:space:]]*//;s/[[:space:]]*$//' | grep -v '^$' > custom-set.txt
          else
            COMP_FILE=$(echo "${{ github.event.inputs.competition_set }}" | awk '{print $1}')
            echo "Using competition set: $COMP_FILE"
            if [ "$COMP_FILE" != "custom-set.txt" ]; then
              cp "$COMP_FILE" custom-set.txt
              echo "Copied $COMP_FILE to custom-set.txt"
            fi
          fi

          echo ""
          echo "Competitions to run:"
          cat custom-set.txt

      - name: Setup Python environment
        run: |
          cd mle-bench

          if [ ! -d "venv" ]; then
            echo "Creating virtual environment..."
            python3 -m venv venv
          fi

          source venv/bin/activate
          echo "Installing mlebench (non-editable mode to avoid import conflicts)..."
          pip install --no-cache-dir .

          echo "✅ Python environment ready"
          echo "   Python: $(which python)"
          echo "   mlebench: $(python -c 'import mlebench; print(mlebench.__file__)')"
          echo "   NOTE: Agent code runs inside Docker container (isolated)"

      - name: Run MLE-Bench
        timeout-minutes: 1400
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          XAI_API_KEY: ${{ secrets.XAI_API_KEY }}
          IMAGE_TAG: agent_v6_kaggle:run-${{ github.run_id }}
          AGENT_ID: agent_v6_kaggle
          RUN_ID: ${{ github.run_id }}
          DRY_RUN: ${{ github.event.inputs.dry_run }}
          REBUILD_IMAGE: ${{ github.event.inputs.rebuild_image }}
        run: |
          cd mle-bench
          source venv/bin/activate

          echo "Starting MLE-Bench with:"
          echo "  Working directory: $(pwd)"
          echo "  AGENT: agent_v6_kaggle (Operand Quant)"
          echo "  IMAGE_TAG: $IMAGE_TAG"
          echo "  RUN_ID: $RUN_ID"
          echo "  DRY_RUN: $DRY_RUN"
          echo "  REBUILD_IMAGE: $REBUILD_IMAGE"
          echo "  Python: $(which python)"
          echo ""

          ./RUN_AGENT_V5_KAGGLE.sh

      - name: Upload results
        if: always() && github.event.inputs.dry_run == 'false'
        uses: actions/upload-artifact@v4
        with:
          name: mle-bench-results-${{ github.ref_name }}-run${{ github.run_number }}
          path: ${{ github.workspace }}/mle-bench/runs/
          retention-days: 30

      - name: Upload agent logs
        if: always() && github.event.inputs.dry_run == 'false'
        uses: actions/upload-artifact@v4
        with:
          name: agent-logs-${{ github.ref_name }}-run${{ github.run_number }}
          path: ${{ github.workspace }}/mle-bench/runs/*/*/agentic.log
          retention-days: 90
          if-no-files-found: warn

      - name: Upload grading report
        if: always() && github.event.inputs.dry_run == 'false'
        uses: actions/upload-artifact@v4
        with:
          name: grading-report-${{ github.ref_name }}-run${{ github.run_number }}
          path: |
            ${{ github.workspace }}/mle-bench/runs/*/*_grading_report.json
            ${{ github.workspace }}/mle-bench/runs/*/results.json
          retention-days: 90

      - name: Display results summary
        if: always() && github.event.inputs.dry_run == 'false'
        run: |
          cd ${{ github.workspace }}/mle-bench

          RUN_GROUP=$(ls -t runs/ 2>/dev/null | head -1)

          if [ -n "$RUN_GROUP" ]; then
            echo ""
            echo "=========================================="
            echo "Results Summary"
            echo "=========================================="
            echo "Run group: $RUN_GROUP"
            echo ""

            GRADING_REPORT=$(find "runs/$RUN_GROUP/" -name "*_grading_report.json" -o -name "results.json" | head -1)

            if [ -n "$GRADING_REPORT" ]; then
              echo "Grading Results:"
              cat "$GRADING_REPORT" | head -50
            else
              echo "⚠️  Grading results not found"
              echo "Files in run directory:"
              ls -la "runs/$RUN_GROUP/"
            fi

            echo ""
            echo "Download artifacts above to see full results"
          else
            echo "⚠️  No results found"
          fi

      - name: Summarize agent logs with Claude
        if: always() && github.event.inputs.dry_run == 'false'
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          CLAUDE_SUMMARY_MODEL: claude-sonnet-4-5-20250929
        run: |
          cd ${{ github.workspace }}/mle-bench
          source venv/bin/activate

          echo "Installing anthropic package for log summarization..."
          pip install --no-cache-dir anthropic

          RUN_GROUP=$(ls -t runs/ 2>/dev/null | head -1)

          if [ -z "$RUN_GROUP" ]; then
            echo "No run group found; skipping log summarization."
            exit 0
          fi

          echo "RUN_GROUP=$RUN_GROUP" >> $GITHUB_ENV

          echo "Running log summarization with Claude..."
          python scripts/summarize_logs.py \
            --run-group "$RUN_GROUP" \
            --output-dir "runs/$RUN_GROUP/log_reviews" \
            --output-zip "runs/$RUN_GROUP/log_reviews.zip" \
            --model "${CLAUDE_SUMMARY_MODEL}"

      - name: Upload log summaries
        if: always() && github.event.inputs.dry_run == 'false'
        uses: actions/upload-artifact@v4
        with:
          name: agent-log-summaries-${{ github.ref_name }}-run${{ github.run_number }}
          path: ${{ github.workspace }}/mle-bench/runs/*/log_reviews.zip
          if-no-files-found: warn
          retention-days: 90

      - name: Display log summaries
        if: always() && github.event.inputs.dry_run == 'false'
        run: |
          cd ${{ github.workspace }}/mle-bench

          RUN_GROUP=$(ls -t runs/ 2>/dev/null | head -1)

          if [ -z "$RUN_GROUP" ]; then
            echo "No run group found; skipping summary display."
            exit 0
          fi

          REVIEWS_DIR="runs/$RUN_GROUP/log_reviews"

          if [ ! -d "$REVIEWS_DIR" ]; then
            echo "⚠️  No log reviews directory found at $REVIEWS_DIR"
            exit 0
          fi

          echo ""
          echo "=========================================="
          echo "Agent Log Summaries"
          echo "=========================================="
          echo "Run group: $RUN_GROUP"
          echo ""

          # Display summaries for each competition
          for run_dir in "$REVIEWS_DIR"/*; do
            if [ -d "$run_dir" ]; then
              RUN_NAME=$(basename "$run_dir")
              SUMMARY_FILE="$run_dir/claude-summary.txt"

              if [ -f "$SUMMARY_FILE" ]; then
                echo "────────────────────────────────────────"
                echo "Competition: $RUN_NAME"
                echo "────────────────────────────────────────"
                cat "$SUMMARY_FILE"
                echo ""
              fi
            fi
          done

          echo "=========================================="
          echo "Download 'agent-log-summaries' artifact for full details"
          echo "=========================================="

      - name: Cleanup Docker
        if: always()
        env:
          RUN_ID: ${{ github.run_id }}
        run: |
          echo "=========================================="
          echo "Docker Cleanup"
          echo "=========================================="

          cd ${{ github.workspace }}
          bash scripts/cleanup-containers.sh || true

          echo ""
          echo "Cleaning up Docker images..."

          docker image rm agent_v6_kaggle:run-${{ github.run_id }} 2>/dev/null || true
          docker container prune -f
          docker image prune -f

          echo ""
          echo "Docker disk usage after cleanup:"
          docker system df

      - name: Cleanup workspace
        if: always()
        run: |
          cd ${{ github.workspace }}/mle-bench

          rm -rf data/ 2>/dev/null || true
          find runs/ -name "*.pth" -o -name "*.pt" -o -name "*.h5" | xargs rm -f 2>/dev/null || true

          echo "Workspace cleanup complete"
