# EstimateTaskDuration Tool - Size-Based Scaling Update

## Summary

Improved the `EstimateTaskDuration` tool to accept **actual file sizes in MB** instead of just categorical "small/medium/large" labels, providing more accurate duration estimates.

## Changes

### 1. New Parameter: `data_size_mb`

**Before:**
```python
{
  "task_type": "load_data",
  "data_size": "large"  # Only "small", "medium", or "large"
}
```

**After:**
```python
{
  "task_type": "load_data",
  "data_size_mb": 5120  # Actual size: 5 GB
}
```

### 2. Intelligent Scaling Algorithm

The tool now uses a **logarithmic scaling function** that accounts for:

- **Sublinear scaling for small files** (< 1GB): Faster than linear due to caching and memory operations
- **Linear scaling for medium files** (~1GB baseline)
- **Superlinear scaling for large files** (> 10GB): Slower than linear due to I/O bottlenecks and memory pressure

**Scaling Formula:**
```python
def _calculate_size_multiplier(size_mb):
    baseline = 1024 MB (1 GB)
    ratio = size_mb / baseline

    if ratio < 1:
        # Sublinear: sqrt scaling
        multiplier = 0.2 + (0.8 * sqrt(ratio))
    else:
        # Superlinear: power scaling
        multiplier = ratio^0.7
```

**Example Multipliers:**

| Size | Multiplier | Meaning |
|------|-----------|---------|
| 10 MB | 0.28x | ~3.5x faster than 1GB |
| 100 MB | 0.45x | ~2x faster than 1GB |
| 1 GB | 1.0x | Baseline |
| 5 GB | 3.09x | ~3x slower than 1GB |
| 10 GB | 5.01x | ~5x slower than 1GB |
| 100 GB | 25.12x | ~25x slower than 1GB |

### 3. Backward Compatibility

The old categorical `data_size` parameter still works:

```python
# Still supported
{
  "task_type": "load_data",
  "data_size": "large"  # Maps to 10 GB
}
```

**Categorical mappings:**
- `"small"` â†’ 100 MB
- `"medium"` â†’ 1024 MB (1 GB)
- `"large"` â†’ 10240 MB (10 GB)

### 4. Human-Readable Size Formatting

Output now shows actual sizes:

**Before:**
```
Parameters:
   â€¢ Data size: large
```

**After:**
```
Parameters:
   â€¢ Data size: 5.00 GB
```

## Examples

### Example 1: Loading a 100 MB CSV file

```python
await tool.execute({
    "task_type": "load_data",
    "data_size_mb": 100
})
```

**Output:**
```
â±ï¸  Task Duration Estimate: load_data

ðŸ“Š Estimated Duration:
   âš¡ Best case:  0s
   ðŸ“ˆ Typical:    2s
   âš ï¸  Worst case: 13s

Parameters:
   â€¢ Data size: 100.0 MB
   â€¢ Complexity: moderate

ðŸ’¡ Recommendations:
   â€¢ This is a quick task - should complete in under a minute
```

### Example 2: Processing a 50 GB dataset

```python
await tool.execute({
    "task_type": "process_large_dataset",
    "data_size_mb": 51200,  # 50 GB
    "complexity": "complex"
})
```

**Output:**
```
â±ï¸  Task Duration Estimate: process_large_dataset

ðŸ“Š Estimated Duration:
   âš¡ Best case:  29m 30s
   ðŸ“ˆ Typical:    2h 34m
   âš ï¸  Worst case: 15h 26m

Parameters:
   â€¢ Data size: 50.00 GB
   â€¢ Complexity: complex

ðŸ’¡ Recommendations:
   â€¢ This is a long-running task - consider running in background
   â€¢ Use Bash(run_in_background=True) and monitor with ReadBashOutput
   â€¢ Set timeout to at least 23h 9m
   â€¢ Consider checkpointing for tasks > 30 minutes
```

## Benefits

1. **More accurate estimates**: Uses actual file size instead of broad categories
2. **Better planning**: Agents can set appropriate timeouts based on data size
3. **Realistic expectations**: Accounts for non-linear scaling of I/O operations
4. **Backward compatible**: Existing code using categorical sizes still works
5. **Transparent**: Shows actual data size in output

## Use Cases

### For Kaggle Competitions (MLE-Bench)

```python
# Agent can check file size and estimate load time
file_size_mb = os.path.getsize("train.csv") / (1024 * 1024)

estimate = await tool.execute({
    "task_type": "load_data",
    "data_size_mb": file_size_mb
})

# Set appropriate timeout based on estimate
# Use typical duration * 1.5 for safety margin
```

### For Data Processing Pipelines

```python
# Estimate total pipeline duration
total_estimate = 0

# Load data
total_estimate += estimate_task("load_data", data_size_mb=5120)

# Feature engineering
total_estimate += estimate_task("feature_engineering", data_size_mb=5120)

# Model training
total_estimate += estimate_task("train_complex_model", complexity="complex")

print(f"Total estimated time: {format_duration(total_estimate)}")
```

## Testing

Run the test suite:

```bash
python test_size_scaling.py
```

This validates:
- âœ… Size-based scaling works correctly
- âœ… Backward compatibility with categorical sizes
- âœ… Multiplier calculations are reasonable
- âœ… Human-readable formatting

## Implementation Details

**Files modified:**
- [agent_v5/tools/estimate_duration.py](agent_v5/tools/estimate_duration.py)

**New functions:**
- `_categorical_to_mb()`: Convert "small/medium/large" to MB
- `_calculate_size_multiplier()`: Logarithmic scaling based on size
- `_format_data_size()`: Human-readable size formatting (MB, GB, TB)

**Changed functions:**
- `execute()`: Now uses `data_size_mb` with fallback to categorical
- `_format_estimate()`: Now displays actual size instead of category

**Lines of code:** +85 lines added for improved functionality
