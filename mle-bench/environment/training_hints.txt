KAGGLE TRAINING HINTS & CRITICAL FAILURE PREVENTION
====================================================

READ BEFORE WRITING train.py - Prevent 90% of failures by following this.

Most training failures are caused by 10-15 common errors. This file lists them with exact fixes.

────────────────────────────────────────────────────────────────────────────────
SECTION 1: MODEL SIZING - CHOOSE BEFORE TRAINING (MOST CRITICAL)
────────────────────────────────────────────────────────────────────────────────

⚠️ MODEL TOO LARGE FOR TIME BUDGET (CAUSES INCOMPLETE TRAINING)
Problem: EfficientNet-B4 + 3 folds takes 50-60 min, exceeds 30-min budget → only 1 fold completes
Real example: Agent chose B4, had to kill after 1 fold, lost ensemble benefit, suboptimal score
Solution: ESTIMATE FIRST, choose appropriate model size

Model Sizing Guide (Image Classification, 224x224, A10 GPU):
  Time Budget  | Recommended Model          | Folds | Epochs | Expected Time
  -------------|----------------------------|-------|--------|---------------
  20-30 min    | EfficientNet-B2, B3        | 3     | 6-8    | ~20-25 min
  30-40 min    | EfficientNet-B3, ResNet50  | 3     | 8-10   | ~30-35 min
  40-60 min    | EfficientNet-B4, ResNeXt50 | 5     | 10     | ~45-55 min
  60+ min      | EfficientNet-B5+, ViT      | 5     | 10-15  | ~60-90 min

Time Estimation Formula:
  total_time = (num_folds × num_epochs × min_per_epoch) + inference_time
  - min_per_epoch varies: B2=0.5 min, B3=1 min, B4=2-3 min, B5=4-5 min (depends on dataset size)
  - inference_time = ~5-8 min (predict.py for test set)
  - Add 20% buffer for overhead (data loading, validation, saving checkpoints)

Example:
  EfficientNet-B4, 3 folds, 10 epochs, 1000 samples/epoch
  Estimate: 3 folds × 10 epochs × 2.5 min/epoch = 75 min → TOO SLOW for 30-min budget
  Fix: Use B3 (1 min/epoch) → 3 × 10 × 1 = 30 min → fits budget

Action: ALWAYS estimate before launching training. If estimate >budget, reduce model size OR folds OR epochs.

────────────────────────────────────────────────────────────────────────────────
SECTION 2: GPU VALIDATION - CHECK IMMEDIATELY AFTER LAUNCH
────────────────────────────────────────────────────────────────────────────────

⚠️ TRAINING ON CPU (SILENT FAILURE, 10-100X SLOWER)
Problem: Library conflict or missing .cuda() → tensors on CPU → training runs but extremely slow
Symptom: GPU memory <10%, validation loss stuck at random baseline (e.g., ln(120)=4.79 for 120 classes)
Solution: Verify GPU usage 60 seconds after launch

Validation Checklist (60 sec after launch):
  1. Check GPU memory: torch.cuda.memory_allocated() / total_memory should be >50%
     Print: f"GPU: {mem_gb:.1f} GB / {total_gb:.1f} GB ({pct:.1f}%)"
  2. If <10% → KILL immediately, debug:
     - Verify: data.cuda(), target.cuda(), model.cuda()
     - Check: torch.cuda.is_available() returns True
     - Check: No library conflicts (e.g., albumentations incompatible with CUDA in some versions)
  3. After 1-2 epochs, check loss vs random baseline:
     - Random baseline = ln(num_classes) for classification
     - If validation loss ≈ baseline (within 0.1) → model not learning → KILL and debug

Always print GPU usage:
  ```python
  # After first batch
  mem = torch.cuda.memory_allocated() / 1024**3
  total = torch.cuda.get_device_properties(0).total_memory / 1024**3
  print(f"GPU: {mem:.1f} GB / {total:.1f} GB ({mem/total*100:.1f}%)")
  # Should show >50% for proper utilization
  ```

────────────────────────────────────────────────────────────────────────────────
SECTION 3: LIBRARY VERSION CONFLICTS
────────────────────────────────────────────────────────────────────────────────

⚠️ ALBUMENTATIONS VERSION CONFLICT
Problem: ImportError: cannot import name 'preserve_channel_dim' from 'albucore.utils'
Solution: Use torchvision.transforms (more stable) OR pin: albumentations==1.3.1 albucore==0.0.7

⚠️ TIMM API CHANGES
Problem: AttributeError: module 'timm' has no attribute 'loss'
Solution: from timm.loss import SoftTargetCrossEntropy (NOT timm.loss.SoftTargetCrossEntropy)

⚠️ MIXED PRECISION TYPE ERROR
Problem: RuntimeError: "nll_loss_out_frame" not implemented for 'Half'
Solution: Loss calculation MUST be inside autocast():
  ```python
  with autocast():
      output = model(data)
      loss = criterion(output, target)  # ← Must be inside, NOT outside
  ```

────────────────────────────────────────────────────────────────────────────────
SECTION 4: BATCH SIZE & DATA LOADING
────────────────────────────────────────────────────────────────────────────────

⚠️ BATCH SIZE TOO SMALL (WASTED GPU)
Problem: GPU memory 5-10%, training very slow
Solution: Increase batch size until GPU memory 70-80%
  - Images 224x224: batch_size=128+ (A10 24GB can handle 192+)
  - Images 384x384: batch_size=64+ (increase to 96+ if no OOM)
  - Tabular: batch_size=4096+
  - Monitor: torch.cuda.memory_allocated() should be 70-80% of total

⚠️ MIXUP/CUTMIX REQUIRES EVEN BATCH SIZE
Problem: AssertionError: Batch size should be even when using this
Solution: Always use drop_last=True:
  ```python
  DataLoader(dataset, batch_size=128, shuffle=True, drop_last=True)
  ```

⚠️ NUM_WORKERS TOO LOW (GPU IDLE)
Problem: GPU utilization <50%, waiting for data
Solution: num_workers=8-12 for high throughput:
  ```python
  DataLoader(dataset, batch_size=128, num_workers=10,
             pin_memory=True, prefetch_factor=4, persistent_workers=True)
  ```

────────────────────────────────────────────────────────────────────────────────
SECTION 5: LABEL ENCODING ERRORS
────────────────────────────────────────────────────────────────────────────────

⚠️ STRING LABELS NOT ENCODED
Problem: ValueError: invalid literal for int() with base 10: 'cat'
Solution: Encode BEFORE training:
  ```python
  from sklearn.preprocessing import LabelEncoder
  le = LabelEncoder()
  train_df['target'] = le.fit_transform(train_df['target'])
  pickle.dump(le, open('label_encoder.pkl', 'wb'))  # Save for predict.py
  ```

⚠️ LABEL ENCODING MISMATCH (TRAIN VS TEST)
Problem: Model outputs don't match expected labels in predict.py
Solution: Fit on ALL unique labels (train + test if available):
  ```python
  # train.py
  all_labels = pd.concat([train_df['target'], test_df['target']]).unique()
  le.fit(all_labels)
  pickle.dump(le, open('label_encoder.pkl', 'wb'))

  # predict.py
  le = pickle.load(open('label_encoder.pkl', 'rb'))
  predictions = le.inverse_transform(pred_classes)
  ```

────────────────────────────────────────────────────────────────────────────────
SECTION 6: DATA LEAKAGE & CV/LB MISMATCH
────────────────────────────────────────────────────────────────────────────────

⚠️ PREPROCESSING BEFORE SPLIT (LEAKAGE)
Problem: CV looks good, LB much worse
Cause: Scaler fitted on full dataset before split
Solution: ALWAYS fit ONLY on training fold:
  ```python
  scaler = StandardScaler()
  X_train_scaled = scaler.fit_transform(X_train)  # ← fit on train only
  X_val_scaled = scaler.transform(X_val)          # ← transform val only
  ```

⚠️ AUGMENTATION ON VALIDATION
Problem: CV score unrealistic (too good)
Solution: Augmentation ONLY for training:
  ```python
  train_transform = transforms.Compose([
      transforms.RandomHorizontalFlip(),  # ← Training only
      transforms.ToTensor()
  ])
  val_transform = transforms.Compose([
      transforms.ToTensor()  # ← No augmentation for validation
  ])
  ```

────────────────────────────────────────────────────────────────────────────────
SECTION 7: MODEL SAVING & CHECKPOINTING
────────────────────────────────────────────────────────────────────────────────

⚠️ ONLY SAVING BEST MODEL (THEN KILLING EARLY)
Problem: Training killed after 1/3 folds, only fold0_best.pth exists → predict.py fails
Real example: Agent killed training at 25 min, only had 1 model, lost ensemble benefit
Solution: ALWAYS save last checkpoint even if not best:
  ```python
  # Save best
  if val_loss < best_loss:
      torch.save(model.state_dict(), f'model_fold{fold}_best.pth')
  # ALSO save last (in case training killed early)
  torch.save(model.state_dict(), f'model_fold{fold}_last.pth')
  ```

predict.py logic:
  ```python
  # Try loading best first, fallback to last
  if os.path.exists(f'model_fold{fold}_best.pth'):
      model.load_state_dict(torch.load(f'model_fold{fold}_best.pth'))
  elif os.path.exists(f'model_fold{fold}_last.pth'):
      model.load_state_dict(torch.load(f'model_fold{fold}_last.pth'))
  ```

────────────────────────────────────────────────────────────────────────────────
SECTION 8: GPU MEMORY & OOM ERRORS
────────────────────────────────────────────────────────────────────────────────

⚠️ OUT OF MEMORY (OOM)
Solution:
  1. Reduce batch_size by 30-50%
  2. Use gradient accumulation to simulate larger batches:
     ```python
     accumulation_steps = 4
     for i, (data, target) in enumerate(loader):
         output = model(data)
         loss = criterion(output, target) / accumulation_steps
         loss.backward()
         if (i + 1) % accumulation_steps == 0:
             optimizer.step()
             optimizer.zero_grad()
     ```
  3. Enable mixed precision (reduces memory by ~50%)

⚠️ ZOMBIE PROCESS (PREVIOUS RUN DIDN'T RELEASE GPU)
Problem: OOM on second run, but first run failed
Real example: Agent's first run OOMed, process didn't exit cleanly, blocked GPU for second run
Solution: Check and kill zombie processes:
  ```bash
  nvidia-smi  # Check GPU processes
  kill -9 <PID>  # Kill zombie process
  ```

────────────────────────────────────────────────────────────────────────────────
SECTION 9: SUBMISSION FORMAT
────────────────────────────────────────────────────────────────────────────────

⚠️ COLUMN NAMES WRONG
Problem: "Expected column 'label' but got 'prediction'"
Solution: Read description.md for exact format, match sample_submission.csv

⚠️ ROW ORDER MATTERS
Problem: Score is 0.0 despite validation passing
Solution: Keep test IDs aligned:
  ```python
  test_df['prediction'] = predictions
  submission = test_df[['id', 'prediction']]  # Preserves order
  submission.to_csv('submission.csv', index=False)
  ```

────────────────────────────────────────────────────────────────────────────────
SECTION 10: TRAINING TEMPLATE (COPY-PASTE READY)
────────────────────────────────────────────────────────────────────────────────

```python
import torch
from torch.cuda.amp import autocast, GradScaler
from sklearn.model_selection import StratifiedKFold

# Config (adjust based on time budget)
BATCH_SIZE = 128  # For images 224x224 on A10 24GB
NUM_WORKERS = 10
N_FOLDS = 3       # Use 3 for 20-30 min budget, 5 for 40+ min
EPOCHS = 8        # Use 6-8 for 20-30 min, 10 for 40+ min
LR = 1e-3

scaler = GradScaler()
skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)

for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
    print(f"\n=== Fold {fold+1}/{N_FOLDS} ===")

    train_loader = DataLoader(
        train_dataset, batch_size=BATCH_SIZE, shuffle=True,
        num_workers=NUM_WORKERS, pin_memory=True,
        prefetch_factor=4, persistent_workers=True, drop_last=True
    )

    model = create_model().cuda()
    optimizer = torch.optim.Adam(model.parameters(), lr=LR)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)
    criterion = torch.nn.CrossEntropyLoss()

    best_score = 0
    patience_counter = 0

    for epoch in range(EPOCHS):
        model.train()
        for i, (data, target) in enumerate(train_loader):
            data, target = data.cuda(), target.cuda()

            with autocast():
                output = model(data)
                loss = criterion(output, target)  # Inside autocast!

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()

            # GPU validation (first batch only)
            if epoch == 0 and i == 0:
                mem = torch.cuda.memory_allocated() / 1024**3
                total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                print(f"GPU: {mem:.1f} GB / {total:.1f} GB ({mem/total*100:.1f}%)")
                if mem/total < 0.10:
                    print("⚠️  CRITICAL: GPU <10% - likely training on CPU!")
                    raise RuntimeError("GPU not being used - check data.cuda() calls")

        # Validation
        model.eval()
        val_loss, val_acc = validate(model, val_loader)
        print(f"Epoch {epoch}: Loss={val_loss:.4f}, Acc={val_acc:.4f}")

        # Loss sanity check (first 2 epochs)
        if epoch < 2:
            random_baseline = torch.log(torch.tensor(num_classes)).item()
            if abs(val_loss - random_baseline) < 0.1:
                print(f"⚠️  Loss ≈ random baseline ({random_baseline:.2f}) - model not learning!")

        # Save checkpoints
        if val_acc > best_score:
            best_score = val_acc
            torch.save(model.state_dict(), f'model_fold{fold}_best.pth')
            patience_counter = 0
        else:
            patience_counter += 1

        # ALWAYS save last (critical if training killed early)
        torch.save(model.state_dict(), f'model_fold{fold}_last.pth')

        # Early stopping
        if patience_counter >= 5:
            print(f"Early stopping at epoch {epoch}")
            break

        scheduler.step()
```

────────────────────────────────────────────────────────────────────────────────
DEBUGGING CHECKLIST BEFORE LAUNCH
────────────────────────────────────────────────────────────────────────────────

□ Time estimated: (folds × epochs × min_per_epoch) < budget?
□ Model size appropriate for time budget (B2/B3 for 30-min, not B4+)?
□ Batch size appropriate (128+ for images, 4096+ for tabular)?
□ Batch size even if using Mixup/CutMix?
□ drop_last=True if using Mixup/CutMix?
□ num_workers=8-12, pin_memory=True, prefetch_factor=4?
□ Mixed precision enabled (autocast/GradScaler)?
□ Loss calculation INSIDE autocast()?
□ Labels encoded to integers if classification?
□ GPU usage print after first batch (should be >50%)?
□ Loss sanity check after 1-2 epochs (vs ln(num_classes))?
□ Saving both best AND last checkpoints?
□ Early stopping patience set (3-5 epochs)?

────────────────────────────────────────────────────────────────────────────────

📌 TOP 3 FAILURE MODES (PREVENT THESE):
  1. Model too large for time budget → incomplete training, suboptimal score
  2. Training on CPU instead of GPU → 10-100x slower, wastes entire budget
  3. Only saving best checkpoint → training killed early, no models for predict.py

────────────────────────────────────────────────────────────────────────────────
