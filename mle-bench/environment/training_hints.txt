KAGGLE COMPETITION TRAINING HINTS & COMMON PITFALLS
====================================================

READ THIS FILE BEFORE WRITING train.py OR ANY TRAINING SCRIPT!

This file contains critical tips to avoid common errors that waste time and compute.
Most failures can be prevented by following these guidelines.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SECTION 1: COMMON LIBRARY VERSION CONFLICTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âš ï¸ ALBUMENTATIONS VERSION CONFLICTS
Problem: ImportError: cannot import name 'preserve_channel_dim' from 'albucore.utils'
Cause: Incompatible versions of albumentations and albucore
Solution:
  - Use torchvision.transforms instead of albumentations (more stable)
  - OR pin versions: albumentations==1.3.1 albucore==0.0.7

âš ï¸ TIMM LIBRARY API CHANGES
Problem: AttributeError: module 'timm' has no attribute 'loss'
Cause: timm.loss not directly accessible in newer versions
Solution:
  - Use: from timm.loss import SoftTargetCrossEntropy
  - NOT: timm.loss.SoftTargetCrossEntropy()

âš ï¸ PYTORCH MIXED PRECISION TYPE ERRORS
Problem: RuntimeError: "nll_loss_out_frame" not implemented for 'Half'
Cause: Loss function expects Float32, but mixed precision passes Float16
Solution:
  - Loss calculation must happen INSIDE autocast():
    ```python
    with autocast():
        output = model(data)
        loss = criterion(output, target)  # â† Must be inside autocast
    ```
  - NOT outside: loss = criterion(output.float(), target) will fail

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SECTION 2: BATCH SIZE & DATA LOADING PITFALLS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âš ï¸ MIXUP/CUTMIX REQUIRES EVEN BATCH SIZE
Problem: AssertionError: Batch size should be even when using this
Cause: Mixup/CutMix requires even batch sizes, but drop_last=False by default
Solution:
  - Always use drop_last=True when using Mixup/CutMix:
    ```python
    train_loader = DataLoader(dataset, batch_size=128,
                             shuffle=True, drop_last=True)  # â† Required!
    ```

âš ï¸ BATCH SIZE TOO SMALL = WASTED GPU
Problem: Training very slow, GPU memory only 5-10% used
Cause: batch_size=32 or smaller wastes GPU compute
Solution:
  - Images 224x224: batch_size=128 minimum (increase to 192 if no OOM)
  - Images 384x384: batch_size=64 minimum (increase to 96 if no OOM)
  - Tabular: batch_size=4096 minimum
  - Monitor: torch.cuda.memory_allocated() should be 70-80% of total

âš ï¸ DATALOADER NUM_WORKERS TOO LOW
Problem: GPU sits idle waiting for data, utilization <50%
Cause: num_workers=0 or too low (e.g., 2-4)
Solution:
  - Use num_workers=8-12 for high throughput:
    ```python
    DataLoader(dataset, batch_size=128, num_workers=10,
               pin_memory=True, prefetch_factor=4, persistent_workers=True)
    ```

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SECTION 3: LABEL ENCODING & TARGET ERRORS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âš ï¸ STRING LABELS NOT ENCODED
Problem: ValueError: invalid literal for int() with base 10: 'cat'
Cause: Target labels are strings, not integers
Solution:
  - Encode labels BEFORE training:
    ```python
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    train_df['target'] = le.fit_transform(train_df['target'])
    # Save encoder: pickle.dump(le, open('label_encoder.pkl', 'wb'))
    ```

âš ï¸ LABEL ENCODING MISMATCH (TRAIN VS TEST)
Problem: Model outputs don't match expected label encoding
Cause: LabelEncoder fitted on train, but test has different encoding
Solution:
  - Fit LabelEncoder on ALL unique labels (train + test if available)
  - Save encoder and reuse in predict.py:
    ```python
    # train.py
    le = LabelEncoder()
    all_labels = pd.concat([train_df['target'], test_df['target']]).unique()
    le.fit(all_labels)
    pickle.dump(le, open('label_encoder.pkl', 'wb'))

    # predict.py
    le = pickle.load(open('label_encoder.pkl', 'rb'))
    predictions = le.inverse_transform(pred_classes)
    ```

âš ï¸ MISSING CLASS IN VALIDATION FOLD
Problem: ValueError: Target values are missing some classes
Cause: StratifiedKFold with imbalanced data, some classes missing in fold
Solution:
  - Use stratified split with min samples check
  - OR use regular KFold (non-stratified) if classes very rare

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SECTION 4: DATA LEAKAGE & CV/LB MISMATCH
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âš ï¸ TARGET LEAKAGE IN FEATURES
Problem: CV=0.99 but LB=0.50 (model not generalizing)
Cause: Features contain information from target variable
Solution:
  - Check for: ID columns, target-derived features, future information
  - Drop: submission_time, user_id, target_encoded features

âš ï¸ PREPROCESSING BEFORE SPLIT (DATA LEAKAGE)
Problem: CV looks good, but LB much worse
Cause: Normalization/scaling fitted on full dataset before split
Solution:
  - ALWAYS fit scalers/normalizers ONLY on training fold:
    ```python
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)  # â† fit on train only
    X_val_scaled = scaler.transform(X_val)          # â† transform val only
    ```

âš ï¸ AUGMENTATION APPLIED TO VALIDATION
Problem: CV score unrealistic (too good)
Cause: Data augmentation applied to validation set
Solution:
  - Augmentation ONLY for training:
    ```python
    train_transform = transforms.Compose([
        transforms.RandomHorizontalFlip(),  # â† Training only
        transforms.RandomRotation(15),
        transforms.ToTensor()
    ])
    val_transform = transforms.Compose([
        transforms.ToTensor()  # â† No augmentation for validation
    ])
    ```

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SECTION 5: MODEL SAVING & CHECKPOINTING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âš ï¸ MODEL NOT SAVED CORRECTLY
Problem: predict.py can't load model, FileNotFoundError
Cause: Model checkpoint not saved, or saved to wrong location
Solution:
  - Save model after each fold:
    ```python
    torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'epoch': epoch,
        'best_score': best_score
    }, f'model_fold{fold}.pth')
    ```
  - Use absolute paths or check current directory

âš ï¸ ONLY SAVING BEST MODEL (THEN KILL TRAINING EARLY)
Problem: Training killed after 3/5 folds, only fold0_best.pth exists
Cause: Only saving when val_loss improves, but killed before all folds done
Solution:
  - ALWAYS save last checkpoint even if not best:
    ```python
    # Save best
    if val_loss < best_loss:
        torch.save(model.state_dict(), f'model_fold{fold}_best.pth')
    # ALSO save last checkpoint
    torch.save(model.state_dict(), f'model_fold{fold}_last.pth')
    ```
  - predict.py should check for both _best.pth and _last.pth

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SECTION 6: PANDAS PERFORMANCE WARNINGS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âš ï¸ DATAFRAME FRAGMENTATION WARNING
Problem: PerformanceWarning: DataFrame is highly fragmented
Cause: Adding columns one-by-one in loop (e.g., OOF predictions)
Solution:
  - Create dict/list first, then build DataFrame:
    ```python
    # BAD: Adds columns one-by-one
    for i, pred in enumerate(predictions):
        oof_df[f'class_{i}'] = pred

    # GOOD: Build dict first, then DataFrame
    pred_dict = {f'class_{i}': pred for i, pred in enumerate(predictions)}
    oof_df = pd.DataFrame(pred_dict)
    # OR use pd.concat([oof_df, pd.DataFrame(pred_dict)], axis=1)
    ```

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SECTION 7: GPU MEMORY & OOM ERRORS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âš ï¸ OUT OF MEMORY (OOM) DURING TRAINING
Problem: RuntimeError: CUDA out of memory
Solution:
  - Reduce batch_size by 30-50%
  - Use gradient accumulation to simulate larger batches:
    ```python
    accumulation_steps = 4
    for i, (data, target) in enumerate(loader):
        output = model(data)
        loss = criterion(output, target) / accumulation_steps
        loss.backward()
        if (i + 1) % accumulation_steps == 0:
            optimizer.step()
            optimizer.zero_grad()
    ```
  - Enable mixed precision (already reduces memory by 50%)

âš ï¸ MEMORY LEAK (MEMORY GROWS OVER TIME)
Problem: First epoch fine, but OOM on epoch 2+
Cause: Tensors not freed, gradients accumulating
Solution:
  - Call optimizer.zero_grad() every iteration
  - Detach loss for logging: loss_value = loss.item() (not loss directly)
  - Use torch.cuda.empty_cache() between epochs if needed

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SECTION 8: SUBMISSION FORMAT ERRORS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âš ï¸ SUBMISSION COLUMN NAMES WRONG
Problem: Validation fails, "Expected column 'label' but got 'prediction'"
Cause: Submission format doesn't match competition requirements
Solution:
  - Read description.md for exact format (e.g., 'id,label' vs 'ImageId,Label')
  - Match sample_submission.csv exactly if it exists

âš ï¸ SUBMISSION ROW ORDER MATTERS
Problem: Validation passes, but score is 0.0
Cause: Predictions in wrong order (not matching test set order)
Solution:
  - Keep test IDs aligned with predictions:
    ```python
    test_df['prediction'] = predictions
    submission = test_df[['id', 'prediction']]  # Preserves order
    submission.to_csv('submission.csv', index=False)
    ```

âš ï¸ MISSING ROWS IN SUBMISSION
Problem: Submission has 8000 rows, expected 10000
Cause: Some test samples skipped or filtered out
Solution:
  - Ensure ALL test samples get predictions (even if NaN/error)
  - Use fillna() for missing predictions

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SECTION 9: KAGGLE-SPECIFIC BEST PRACTICES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âœ… ALWAYS USE CROSS-VALIDATION
  - K-Fold (k=5 or k=3) for robust evaluation
  - StratifiedKFold for classification (maintains class balance)
  - GroupKFold if samples are grouped (e.g., time series, same user)

âœ… ENSEMBLE MULTIPLE FOLDS
  - Average predictions from all folds for final submission
  - Weighted average if some folds perform better
  - Reduces variance, usually improves LB score

âœ… TRACK OUT-OF-FOLD (OOF) PREDICTIONS
  - Save predictions for each fold on its validation set
  - Combine all OOF predictions â†’ full train set predictions
  - OOF score â‰ˆ LB score (if no leakage)

âœ… USE PRETRAINED MODELS
  - ImageNet pretrained for images (timm library)
  - BERT/RoBERTa pretrained for NLP
  - Faster convergence, better performance than random init

âœ… LEARNING RATE SCHEDULING
  - CosineAnnealingLR or ReduceLROnPlateau
  - Helps model converge better in later epochs
  - Example: optimizer = Adam(lr=1e-3), scheduler = CosineAnnealingLR(optimizer, T_max=epochs)

âœ… EARLY STOPPING
  - Stop training if validation loss doesn't improve for 3-5 epochs
  - Saves time, prevents overfitting
  - Example: if no improvement for 5 epochs â†’ break

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SECTION 10: DEBUGGING CHECKLIST BEFORE TRAINING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Before running train.py, verify:

â–¡ Batch size is even if using Mixup/CutMix
â–¡ drop_last=True if using Mixup/CutMix
â–¡ num_workers=8-12 for DataLoader
â–¡ pin_memory=True, prefetch_factor=4, persistent_workers=True
â–¡ Batch size appropriate for GPU (128+ for images, 4096+ for tabular)
â–¡ Mixed precision enabled (autocast/GradScaler)
â–¡ Loss calculation INSIDE autocast() context
â–¡ Labels encoded to integers if classification
â–¡ Augmentation only applied to training set
â–¡ Scaler/normalizer fitted only on training fold
â–¡ Model saving logic includes both best AND last checkpoints
â–¡ GPU memory monitoring prints (torch.cuda.memory_allocated())
â–¡ Cross-validation strategy chosen (StratifiedKFold/GroupKFold)
â–¡ Early stopping patience set (3-5 epochs)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SECTION 11: QUICK REFERENCE - TRAINING TEMPLATE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

```python
import torch
from torch.cuda.amp import autocast, GradScaler
from torch.utils.data import DataLoader
from sklearn.model_selection import StratifiedKFold

# Config
BATCH_SIZE = 128  # For images 224x224 on A10 24GB
NUM_WORKERS = 10
N_FOLDS = 5
EPOCHS = 10
LR = 1e-3

# Mixed precision
scaler = GradScaler()

# Cross-validation
skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)

for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
    print(f"\n=== Fold {fold+1}/{N_FOLDS} ===")

    # DataLoaders
    train_loader = DataLoader(
        train_dataset, batch_size=BATCH_SIZE, shuffle=True,
        num_workers=NUM_WORKERS, pin_memory=True,
        prefetch_factor=4, persistent_workers=True, drop_last=True
    )
    val_loader = DataLoader(
        val_dataset, batch_size=BATCH_SIZE, shuffle=False,
        num_workers=NUM_WORKERS, pin_memory=True
    )

    # Model, optimizer, scheduler
    model = create_model().cuda()
    optimizer = torch.optim.Adam(model.parameters(), lr=LR)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)
    criterion = torch.nn.CrossEntropyLoss()

    # Print GPU memory after first batch
    first_batch = True

    best_score = 0
    patience_counter = 0

    for epoch in range(EPOCHS):
        model.train()
        for i, (data, target) in enumerate(train_loader):
            data, target = data.cuda(), target.cuda()

            with autocast():
                output = model(data)
                loss = criterion(output, target)  # Inside autocast!

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()

            if first_batch:
                mem = torch.cuda.memory_allocated() / 1024**3
                total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                print(f"GPU Memory: {mem:.2f} GB / {total:.1f} GB ({mem/total*100:.1f}%)")
                first_batch = False

        # Validation
        model.eval()
        val_loss, val_acc = validate(model, val_loader)

        # Save checkpoints
        if val_acc > best_score:
            best_score = val_acc
            torch.save(model.state_dict(), f'model_fold{fold}_best.pth')
            patience_counter = 0
        else:
            patience_counter += 1

        # Always save last
        torch.save(model.state_dict(), f'model_fold{fold}_last.pth')

        # Early stopping
        if patience_counter >= 5:
            print(f"Early stopping at epoch {epoch}")
            break

        scheduler.step()

print("Training complete!")
```

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“Œ REMEMBER: Read this file BEFORE writing train.py to avoid these pitfalls!

Most training failures are caused by issues listed above. Following these guidelines
will save hours of debugging time.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
