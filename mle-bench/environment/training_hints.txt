KAGGLE TRAINING HINTS & CRITICAL FAILURE PREVENTION
====================================================

READ BEFORE WRITING train.py - Prevent 90% of failures by following this.

Most training failures are caused by 10-15 common errors. This file lists them with exact fixes.

────────────────────────────────────────────────────────────────────────────────
SECTION 1: MODEL SIZING - CHOOSE BEFORE TRAINING (MOST CRITICAL)
────────────────────────────────────────────────────────────────────────────────

⚠️ MODEL TOO LARGE FOR TIME BUDGET (CAUSES INCOMPLETE TRAINING)
Problem: EfficientNet-B4 + 3 folds takes 50-60 min, exceeds 30-min budget → only 1 fold completes
Real example: Agent chose B4, had to kill after 1 fold, lost ensemble benefit, suboptimal score
Solution: ESTIMATE FIRST, choose appropriate model size

Model Sizing Guide (Image Classification, 224x224, A10 GPU):
  Time Budget  | Recommended Model          | Folds | Epochs | Expected Time
  -------------|----------------------------|-------|--------|---------------
  20-30 min    | EfficientNet-B2, B3        | 3     | 6-8    | ~20-25 min
  30-40 min    | EfficientNet-B3, ResNet50  | 3     | 8-10   | ~30-35 min
  40-60 min    | EfficientNet-B4, ResNeXt50 | 5     | 10     | ~45-55 min
  60+ min      | EfficientNet-B5+, ViT      | 5     | 10-15  | ~60-90 min

Time Estimation Formula:
  total_time = (num_folds × num_epochs × min_per_epoch) + inference_time
  - min_per_epoch varies: B2=0.5 min, B3=1 min, B4=2-3 min, B5=4-5 min (depends on dataset size)
  - inference_time = ~5-8 min (predict.py for test set)
  - Add 20% buffer for overhead (data loading, validation, saving checkpoints)

Example:
  EfficientNet-B4, 3 folds, 10 epochs, 1000 samples/epoch
  Estimate: 3 folds × 10 epochs × 2.5 min/epoch = 75 min → TOO SLOW for 30-min budget
  Fix: Use B3 (1 min/epoch) → 3 × 10 × 1 = 30 min → fits budget

Action: ALWAYS estimate before launching training. If estimate >budget, reduce model size OR folds OR epochs.

────────────────────────────────────────────────────────────────────────────────
SECTION 2: GPU VALIDATION - CHECK IMMEDIATELY AFTER LAUNCH
────────────────────────────────────────────────────────────────────────────────

⚠️ TRAINING ON CPU (SILENT FAILURE, 10-100X SLOWER)
Problem: Library conflict or missing .cuda() → tensors on CPU → training runs but extremely slow
Symptom: GPU memory <10%, validation loss stuck at random baseline (e.g., ln(120)=4.79 for 120 classes)
Solution: Verify GPU usage 60 seconds after launch

Validation Checklist (60 sec after launch):
  1. Check GPU memory: torch.cuda.memory_allocated() / total_memory should be >50%
     Print: f"GPU: {mem_gb:.1f} GB / {total_gb:.1f} GB ({pct:.1f}%)"
  2. If <10% → KILL immediately, debug:
     - Verify: data.cuda(), target.cuda(), model.cuda()
     - Check: torch.cuda.is_available() returns True
     - Check: No library conflicts (e.g., albumentations incompatible with CUDA in some versions)
  3. After 1-2 epochs, check loss vs random baseline:
     - Random baseline = ln(num_classes) for classification
     - If validation loss ≈ baseline (within 0.1) → model not learning → KILL and debug

Always print GPU usage:
  ```python
  # After first batch
  mem = torch.cuda.memory_allocated() / 1024**3
  total = torch.cuda.get_device_properties(0).total_memory / 1024**3
  print(f"GPU: {mem:.1f} GB / {total:.1f} GB ({mem/total*100:.1f}%)")
  # Should show >50% for proper utilization
  ```

────────────────────────────────────────────────────────────────────────────────
SECTION 2A: TRAINING SMOKE TEST (RUN BEFORE FULL TRAINING)
────────────────────────────────────────────────────────────────────────────────

⚡ ALWAYS TEST 1 BATCH BEFORE LAUNCHING FULL TRAINING
Problem: Agent launches 4-epoch training, hangs at initialization for 2 minutes, wastes time
Real example: ranzcr-clip competition - 6 training attempts all hung during model init, zero actual training completed
Solution: Run smoke test first (30 seconds) to validate entire pipeline

Smoke Test Template (add to train.py):
  ```python
  # SMOKE TEST: Validate pipeline before full training
  print("🔥 Running smoke test (1 batch)...")
  model = create_model().cuda()
  train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,
                           num_workers=0, shuffle=True)  # Start with 0 workers

  # Test single forward pass
  data, target = next(iter(train_loader))
  data, target = data.cuda(), target.cuda()

  with autocast():
      output = model(data)
      loss = criterion(output, target)

  # Test backward pass
  scaler.scale(loss).backward()

  # Verify GPU usage
  mem = torch.cuda.memory_allocated() / 1024**3
  print(f"✓ Smoke test passed! GPU: {mem:.1f} GB, Loss: {loss.item():.4f}")
  print("🚀 Starting full training...\n")
  ```

Why this catches failures early:
  - DataLoader issues (JPEG decode errors, file I/O problems)
  - Model initialization hangs (timm download, cache issues)
  - GPU transfer failures (CUDA errors)
  - Memory errors (OOM on first batch = reduce batch_size immediately)
  - Library incompatibilities (albumentations, mixed precision errors)

Action: ALWAYS run smoke test. If it fails or hangs >30 seconds, debug BEFORE launching full training.

────────────────────────────────────────────────────────────────────────────────
SECTION 2B: AGGRESSIVE TIMEOUT POLICY (KILL HUNG PROCESSES IMMEDIATELY)
────────────────────────────────────────────────────────────────────────────────

⚠️ PROCESS TIMEOUT - DON'T WATCH HUNG TRAINING
Problem: Training process runs 120+ seconds with no output, agent keeps checking, wastes time
Real example: ranzcr-clip - agent made 15+ CheckProcess calls watching hung processes (PIDs 352, 2730, 5107, 5514, 5854, 8250), wasted 10 minutes
Solution: Kill after 60 seconds of no output, pivot to fallback immediately

Timeout Rules:
  - If process produces NO stdout/stderr output for 60 consecutive seconds → KILL
  - If process shows initialization messages but no "Epoch 1" after 90 seconds → KILL
  - If smoke test hangs >30 seconds → KILL

  Don't wait to see if it recovers - it won't. Hung processes stay hung.

Kill and Pivot Strategy:
  ```bash
  # After 60 sec with no output
  kill -9 <PID>

  # Immediately try fallback (in order):
  # 1. Reduce complexity: num_workers=0, batch_size/2, epochs/2
  # 2. If still hangs: Switch library (torchvision if albumentations failed)
  # 3. If still hangs: Pivot to simple sklearn baseline (guarantee SOME model)
  ```

Time-Boxing Debugging:
  - Single error: 5 minute max per issue
  - After 5 min with no progress, switch approach entirely
  - After 3 failed training attempts, pivot to simplest baseline that works
  - After 15 min total debugging, launch parallel fallback baseline

Examples of Productive Pivots:
  - albumentations ImportError (3 min debugging) → switch to torchvision.transforms ✓
  - DataLoader hangs with num_workers=12 (2 min) → try num_workers=0 ✓
  - Model init hangs downloading weights (2 min) → use different model or cached weights ✓
  - OOM errors (1 min) → reduce batch_size by 50% ✓

Action: Set hard time limits. Kill hung processes aggressively. Pivot to working alternatives fast.

────────────────────────────────────────────────────────────────────────────────
SECTION 2C: MANDATORY SUBMISSION VERIFICATION (PREVENTS NULL SCORES)
────────────────────────────────────────────────────────────────────────────────

🔴 CRITICAL: ALWAYS VERIFY SUBMISSION FILE EXISTS BEFORE DECLARING SUCCESS
Problem: Agent completes work but never creates submission.csv → null score (0 points)
Real examples: leaf-classification (no submission), siim-isic (context overflow, no submission)
Solution: MANDATORY verification checks before ending competition

Submission Verification Checklist (RUN BEFORE EXITING):
  ```python
  import os, pandas as pd, numpy as np

  # STEP 1: File exists
  submission_path = '/home/submission/submission.csv'
  if not os.path.exists(submission_path):
      print("❌ CRITICAL: submission.csv not created!")
      print("Creating emergency baseline...")
      # Emergency baseline creation code here
      raise AssertionError("Must create submission before exiting")

  # STEP 2: File is readable
  try:
      df = pd.read_csv(submission_path)
      print(f"✅ Submission file readable: {df.shape}")
  except Exception as e:
      print(f"❌ CRITICAL: Cannot read submission.csv: {e}")
      raise

  # STEP 3: Basic shape validation
  print(f"Submission shape: {df.shape[0]} rows, {df.shape[1]} columns")
  # Note: Don't assert row count - test set size varies
  # Just verify file is not empty
  if len(df) == 0:
      print("❌ CRITICAL: Submission is empty!")
      raise AssertionError("Submission has 0 rows")

  # STEP 4: Check for NaN/inf values
  if df.isnull().any().any():
      print(f"⚠️  WARNING: Submission has NaN values")
      print(df.isnull().sum())
      # Fill NaNs with safe default (0.5 for probabilities)
      df = df.fillna(0.5)
      df.to_csv(submission_path, index=False)
      print("✅ Filled NaN values with 0.5")

  if np.isinf(df.select_dtypes(include=np.number)).any().any():
      print(f"⚠️  WARNING: Submission has infinity values")
      # Clip infinities
      numeric_cols = df.select_dtypes(include=np.number).columns
      df[numeric_cols] = df[numeric_cols].replace([np.inf, -np.inf], [1.0, 0.0])
      df.to_csv(submission_path, index=False)
      print("✅ Clipped infinity values")

  print("✅ ✅ ✅ SUBMISSION VERIFIED SUCCESSFULLY ✅ ✅ ✅")
  ```

Emergency Baseline (if submission missing):
  ```python
  # Create minimal baseline - better than null score
  import pandas as pd

  # Load test data to get IDs
  test_df = pd.read_csv('/home/data/test.csv')  # Adjust path

  # Strategy 1: Predict class frequencies (for classification)
  train_df = pd.read_csv('/home/data/train.csv')
  target_col = 'target'  # Adjust column name
  class_freq = train_df[target_col].value_counts(normalize=True)

  # Create submission with most frequent class or 0.5 probabilities
  submission = pd.DataFrame({'id': test_df['id']})
  for col in class_freq.index:
      submission[col] = class_freq[col]  # Or use 0.5 for binary

  submission.to_csv('/home/submission/submission.csv', index=False)
  print("✅ Emergency baseline created")
  ```

MANDATORY ACTION: Add verification code to END of predict.py or as separate verify_submission.py script.
NEVER declare competition complete without running verification.

────────────────────────────────────────────────────────────────────────────────
SECTION 3: LIBRARY VERSION CONFLICTS
────────────────────────────────────────────────────────────────────────────────

⚠️ ALBUMENTATIONS VERSION CONFLICT
Problem: ImportError: cannot import name 'preserve_channel_dim' from 'albucore.utils'
Solution: Use torchvision.transforms (more stable) OR pin: albumentations==1.3.1 albucore==0.0.7

⚠️ TIMM API CHANGES
Problem: AttributeError: module 'timm' has no attribute 'loss'
Solution: from timm.loss import SoftTargetCrossEntropy (NOT timm.loss.SoftTargetCrossEntropy)

⚠️ MIXED PRECISION TYPE ERROR
Problem: RuntimeError: "nll_loss_out_frame" not implemented for 'Half'
Solution: Loss calculation MUST be inside autocast():
  ```python
  with autocast():
      output = model(data)
      loss = criterion(output, target)  # ← Must be inside, NOT outside
  ```

────────────────────────────────────────────────────────────────────────────────
SECTION 3A: FASTAI & KERAS QUICK START OPTIONS
────────────────────────────────────────────────────────────────────────────────

FASTAI (vision/tabular rapid baselines)
  When to use:
    - Need a strong image or tabular baseline in <30 minutes with minimal boilerplate.
    - Want automatic learning-rate finder, fit-one-cycle scheduling, and callback stack before diving into custom PyTorch.
  How to run:
    - Images: `from fastai.vision.all import *`; create `DataBlock` → `Learner` (resnet34/resnet50) → `learn.fit_one_cycle(epochs)`.
    - Tabular: `fastai.tabular.all` with `TabularPandas` + `TabularLearner` gives competitive baselines prior to LightGBM tuning.
    - Export: `learn.export()` or `learn.model.state_dict()` so you can ensemble with later PyTorch models.
  Tips:
    - Fastai auto-augments aggressively; trim augmentations if GPU <60% or time budget tight.
    - Review normalization/categorical encoders to ensure they match Kaggle CV split (fit on train folds only).

KERAS / TENSORFLOW (text & multi-input flexibility)
  When to use:
    - Multi-label NLP or mixed-modal problems where Keras Functional API simplifies branching heads.
    - Need to fuse TextVectorization, embeddings, and metadata with minimal custom training loop code.
  How to run:
    - Text: `tf.keras.Sequential` with `TextVectorization` + Dense/Transformer layers, callbacks `ModelCheckpoint` + `ReduceLROnPlateau`.
    - Mixed inputs: Functional API to merge text + tabular branches; use `tf.keras.mixed_precision.set_global_policy("mixed_float16")` on NVIDIA GPUs.
    - Export: `model.save("model.keras")` or `tf.saved_model.save` for inference in predict.py.
  Tips:
    - Cap sequence length at 128/256 unless budget allows (remember mean column-wise AUC tasks prefer throughput).
    - Monitor GPU usage via `tf.config.experimental.get_memory_usage("GPU:0")`; if 0 bytes, training silently fell back to CPU.

────────────────────────────────────────────────────────────────────────────────
SECTION 4: BATCH SIZE & DATA LOADING
────────────────────────────────────────────────────────────────────────────────

⚠️ BATCH SIZE TOO SMALL (WASTED GPU)
Problem: GPU memory 5-10%, training very slow
Solution: Increase batch size until GPU memory 70-80%
  - Images 224x224: batch_size=128+ (A10 24GB can handle 192+)
  - Images 384x384: batch_size=64+ (increase to 96+ if no OOM)
  - Tabular: batch_size=4096+
  - Monitor: torch.cuda.memory_allocated() should be 70-80% of total

⚠️ MIXUP/CUTMIX REQUIRES EVEN BATCH SIZE
Problem: AssertionError: Batch size should be even when using this
Solution: Always use drop_last=True:
  ```python
  DataLoader(dataset, batch_size=128, shuffle=True, drop_last=True)
  ```

⚠️ NUM_WORKERS TOO LOW (GPU IDLE)
Problem: GPU utilization <50%, waiting for data
Solution: num_workers=8-12 for high throughput:
  ```python
  DataLoader(dataset, batch_size=128, num_workers=10,
             pin_memory=True, prefetch_factor=4, persistent_workers=True)
  ```

────────────────────────────────────────────────────────────────────────────────
SECTION 5: LABEL ENCODING ERRORS
────────────────────────────────────────────────────────────────────────────────

⚠️ STRING LABELS NOT ENCODED
Problem: ValueError: invalid literal for int() with base 10: 'cat'
Solution: Encode BEFORE training:
  ```python
  from sklearn.preprocessing import LabelEncoder
  le = LabelEncoder()
  train_df['target'] = le.fit_transform(train_df['target'])
  pickle.dump(le, open('label_encoder.pkl', 'wb'))  # Save for predict.py
  ```

⚠️ LABEL ENCODING MISMATCH (TRAIN VS TEST)
Problem: Model outputs don't match expected labels in predict.py
Solution: Fit on ALL unique labels (train + test if available):
  ```python
  # train.py
  all_labels = pd.concat([train_df['target'], test_df['target']]).unique()
  le.fit(all_labels)
  pickle.dump(le, open('label_encoder.pkl', 'wb'))

  # predict.py
  le = pickle.load(open('label_encoder.pkl', 'rb'))
  predictions = le.inverse_transform(pred_classes)
  ```

────────────────────────────────────────────────────────────────────────────────
SECTION 6: DATA LEAKAGE & CV/LB MISMATCH
────────────────────────────────────────────────────────────────────────────────

⚠️ PREPROCESSING BEFORE SPLIT (LEAKAGE)
Problem: CV looks good, LB much worse
Cause: Scaler fitted on full dataset before split
Solution: ALWAYS fit ONLY on training fold:
  ```python
  scaler = StandardScaler()
  X_train_scaled = scaler.fit_transform(X_train)  # ← fit on train only
  X_val_scaled = scaler.transform(X_val)          # ← transform val only
  ```

⚠️ AUGMENTATION ON VALIDATION
Problem: CV score unrealistic (too good)
Solution: Augmentation ONLY for training:
  ```python
  train_transform = transforms.Compose([
      transforms.RandomHorizontalFlip(),  # ← Training only
      transforms.ToTensor()
  ])
  val_transform = transforms.Compose([
      transforms.ToTensor()  # ← No augmentation for validation
  ])
  ```

────────────────────────────────────────────────────────────────────────────────
SECTION 7: MODEL SAVING & CHECKPOINTING
────────────────────────────────────────────────────────────────────────────────

⚠️ ONLY SAVING BEST MODEL (THEN KILLING EARLY)
Problem: Training killed after 1/3 folds, only fold0_best.pth exists → predict.py fails
Real example: Agent killed training at 25 min, only had 1 model, lost ensemble benefit
Solution: ALWAYS save last checkpoint even if not best:
  ```python
  # Save best
  if val_loss < best_loss:
      torch.save(model.state_dict(), f'model_fold{fold}_best.pth')
  # ALSO save last (in case training killed early)
  torch.save(model.state_dict(), f'model_fold{fold}_last.pth')
  ```

predict.py logic:
  ```python
  # Try loading best first, fallback to last
  if os.path.exists(f'model_fold{fold}_best.pth'):
      model.load_state_dict(torch.load(f'model_fold{fold}_best.pth'))
  elif os.path.exists(f'model_fold{fold}_last.pth'):
      model.load_state_dict(torch.load(f'model_fold{fold}_last.pth'))
  ```

────────────────────────────────────────────────────────────────────────────────
SECTION 8: GPU MEMORY & OOM ERRORS
────────────────────────────────────────────────────────────────────────────────

⚠️ OUT OF MEMORY (OOM)
Solution:
  1. Reduce batch_size by 30-50%
  2. Use gradient accumulation to simulate larger batches:
     ```python
     accumulation_steps = 4
     for i, (data, target) in enumerate(loader):
         output = model(data)
         loss = criterion(output, target) / accumulation_steps
         loss.backward()
         if (i + 1) % accumulation_steps == 0:
             optimizer.step()
             optimizer.zero_grad()
     ```
  3. Enable mixed precision (reduces memory by ~50%)

⚠️ ZOMBIE PROCESS (PREVIOUS RUN DIDN'T RELEASE GPU)
Problem: OOM on second run, but first run failed
Real example: Agent's first run OOMed, process didn't exit cleanly, blocked GPU for second run
Solution: Check and kill zombie processes:
  ```bash
  nvidia-smi  # Check GPU processes
  kill -9 <PID>  # Kill zombie process
  ```

────────────────────────────────────────────────────────────────────────────────
SECTION 9: SUBMISSION FORMAT
────────────────────────────────────────────────────────────────────────────────

⚠️ COLUMN NAMES WRONG
Problem: "Expected column 'label' but got 'prediction'"
Solution: Read description.md for exact format, match sample_submission.csv

⚠️ ROW ORDER MATTERS
Problem: Score is 0.0 despite validation passing
Solution: Keep test IDs aligned:
  ```python
  test_df['prediction'] = predictions
  submission = test_df[['id', 'prediction']]  # Preserves order
  submission.to_csv('submission.csv', index=False)
  ```

────────────────────────────────────────────────────────────────────────────────
SECTION 10: TRAINING TEMPLATE (COPY-PASTE READY)
────────────────────────────────────────────────────────────────────────────────

```python
import torch
from torch.cuda.amp import autocast, GradScaler
from sklearn.model_selection import StratifiedKFold

# Config (adjust based on time budget)
BATCH_SIZE = 128  # For images 224x224 on A10 24GB
NUM_WORKERS = 10
N_FOLDS = 3       # Use 3 for 20-30 min budget, 5 for 40+ min
EPOCHS = 8        # Use 6-8 for 20-30 min, 10 for 40+ min
LR = 1e-3

scaler = GradScaler()
skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)

for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
    print(f"\n=== Fold {fold+1}/{N_FOLDS} ===")

    train_loader = DataLoader(
        train_dataset, batch_size=BATCH_SIZE, shuffle=True,
        num_workers=NUM_WORKERS, pin_memory=True,
        prefetch_factor=4, persistent_workers=True, drop_last=True
    )

    model = create_model().cuda()
    optimizer = torch.optim.Adam(model.parameters(), lr=LR)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)
    criterion = torch.nn.CrossEntropyLoss()

    best_score = 0
    patience_counter = 0

    for epoch in range(EPOCHS):
        model.train()
        for i, (data, target) in enumerate(train_loader):
            data, target = data.cuda(), target.cuda()

            with autocast():
                output = model(data)
                loss = criterion(output, target)  # Inside autocast!

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()

            # GPU validation (first batch only)
            if epoch == 0 and i == 0:
                mem = torch.cuda.memory_allocated() / 1024**3
                total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                print(f"GPU: {mem:.1f} GB / {total:.1f} GB ({mem/total*100:.1f}%)")
                if mem/total < 0.10:
                    print("⚠️  CRITICAL: GPU <10% - likely training on CPU!")
                    raise RuntimeError("GPU not being used - check data.cuda() calls")

        # Validation
        model.eval()
        val_loss, val_acc = validate(model, val_loader)
        print(f"Epoch {epoch}: Loss={val_loss:.4f}, Acc={val_acc:.4f}")

        # Loss sanity check (first 2 epochs)
        if epoch < 2:
            random_baseline = torch.log(torch.tensor(num_classes)).item()
            if abs(val_loss - random_baseline) < 0.1:
                print(f"⚠️  Loss ≈ random baseline ({random_baseline:.2f}) - model not learning!")

        # Save checkpoints
        if val_acc > best_score:
            best_score = val_acc
            torch.save(model.state_dict(), f'model_fold{fold}_best.pth')
            patience_counter = 0
        else:
            patience_counter += 1

        # ALWAYS save last (critical if training killed early)
        torch.save(model.state_dict(), f'model_fold{fold}_last.pth')

        # Early stopping
        if patience_counter >= 5:
            print(f"Early stopping at epoch {epoch}")
            break

        scheduler.step()
```

────────────────────────────────────────────────────────────────────────────────
DEBUGGING CHECKLIST BEFORE LAUNCH
────────────────────────────────────────────────────────────────────────────────

□ Time estimated: (folds × epochs × min_per_epoch) < budget?
□ Model size appropriate for time budget (B2/B3 for 30-min, not B4+)?
□ Batch size appropriate (128+ for images, 4096+ for tabular)?
□ Batch size even if using Mixup/CutMix?
□ drop_last=True if using Mixup/CutMix?
□ num_workers=8-12, pin_memory=True, prefetch_factor=4?
□ Mixed precision enabled (autocast/GradScaler)?
□ Loss calculation INSIDE autocast()?
□ Labels encoded to integers if classification?
□ GPU usage print after first batch (should be >50%)?
□ Loss sanity check after 1-2 epochs (vs ln(num_classes))?
□ Saving both best AND last checkpoints?
□ Early stopping patience set (3-5 epochs)?

────────────────────────────────────────────────────────────────────────────────

📌 TOP 3 FAILURE MODES (PREVENT THESE):
  1. Model too large for time budget → incomplete training, suboptimal score
  2. Training on CPU instead of GPU → 10-100x slower, wastes entire budget
  3. Only saving best checkpoint → training killed early, no models for predict.py

────────────────────────────────────────────────────────────────────────────────
