KAGGLE GRANDMASTER PLAYBOOK - BATTLE-TESTED WINNING STRATEGIES
================================================================

READ THIS FIRST - Before writing any strategy or training code.

This is a distilled synthesis of winning Kaggle solutions. No theory, just what works.

────────────────────────────────────────────────────────────────────────────────
PART I: UNIVERSAL WORKFLOW - APPLIES TO ALL COMPETITIONS
────────────────────────────────────────────────────────────────────────────────

1. MINDSET & COMMUNITY
   - Progress is incremental. Breakthroughs = many small improvements.
   - Read forums/notebooks FIRST. Top solutions are public knowledge.
   - Monitor for new architectures (e.g., DeBERTa dominated NLP after one forum post).

2. FAST EXPERIMENTATION PIPELINE
   - #1 success factor = experiments per day, not hours per experiment.
   - Use GPU-accelerated libraries: cuDF/cuML for tabular, mixed precision for deep learning.
   - Build simple baselines FIRST (logistic regression, quick GBDT) before complex models.
   - Advanced techniques (stacking, pseudo-labeling) only feasible with fast pipeline.

3. CROSS-VALIDATION STRATEGY (MOST IMPORTANT)
   - NEVER trust public leaderboard - it's a tiny subset, causes "shake-ups".
   - "Trust your CV" - local validation must match test data structure.
   - Choose CV strategy by data type:

   Data Type                  | CV Strategy           | Key Consideration
   ---------------------------|-----------------------|------------------
   Standard classification    | KFold (k=3-5)         | General purpose
   Imbalanced classes         | StratifiedKFold       | Preserves class ratios
   Grouped data (user/patient)| GroupKFold            | No group in train+val
   Time series                | TimeSeriesSplit       | Val always after train

   - Use k=3 for speed (20-30 min budget), k=5 for robustness (if time allows).

────────────────────────────────────────────────────────────────────────────────
PART II: DOMAIN-SPECIFIC ARCHITECTURES - WHAT ACTUALLY WINS
────────────────────────────────────────────────────────────────────────────────

TABULAR DATA
------------
Model Choice:
  - GBDTs dominate: LightGBM (fastest, large data), XGBoost (stable baseline), CatBoost (categorical features).
  - LightGBM default for speed-critical scenarios (30-min budget).
  - Hybrid: GBDT + NN (GRU/Transformer) for +0.5% boost (adds ~5-10 min).

Feature Engineering (most important):
  - Heavy manual engineering beats automated tools.
  - Create: interactions (A*B, A/B), polynomials (A²), group aggregations (mean/std by category).
  - Test each feature against CV before adding.

COMPUTER VISION
---------------
Architecture by Task:
  Classification: EfficientNet (B0-B3 for speed, B4+ for accuracy), ResNeXt, ViT
  Detection:      YOLOv5/v8, Faster R-CNN
  Segmentation:   U-Net, FPN

Key Decision - Model Size vs Time Budget:
  30-min budget → EfficientNet-B2/B3 max (B4+ too slow for 3-fold CV)
  60-min budget → EfficientNet-B4/B5
  Medical imaging → ResNet/ResNeXt (better than EfficientNet for grayscale)

Preprocessing:
  - Medical (DICOM): Convert to Hounsfield Units, resample to isotropic, segment ROI.
  - CLAHE for contrast enhancement (medical/low-contrast images).

Augmentation (critical):
  - Standard: RandomHorizontalFlip, RandomRotation, ColorJitter (torchvision stable, albumentations faster).
  - Advanced: MixUp/CutMix (forces smooth decision boundaries, +1-2% accuracy).
  - NEVER augment validation set.

NATURAL LANGUAGE PROCESSING
----------------------------
Model Evolution (in order of strength):
  BERT → RoBERTa → DeBERTa (current best for most tasks)

Strategy:
  - Fine-tune pretrained model (always start here).
  - Monitor forums for new models - NLP is "architectural meta-game".
  - Long sequences: Use Longformer or add BiLSTM on top of Transformer embeddings.
  - Large models + tight constraints: Use knowledge distillation (teacher→student).

TIME SERIES
-----------
  - Transform to tabular + use GBDTs (wins most competitions).
  - Features: time-based (hour/day/month/holiday), lags (past values), rolling stats (mean/std over window).
  - CV: TimeSeriesSplit (mandatory to prevent future leakage).

AUDIO
-----
  - Convert to image: Mel-spectrograms (maps to human perception).
  - Apply CV models: ResNet/EfficientNet on spectrograms.

RECOMMENDER SYSTEMS
-------------------
  - Hybrid models win: Collaborative filtering + content-based filtering.
  - Deep learning: Embedding layers for users/items (learn latent preferences).

────────────────────────────────────────────────────────────────────────────────
PART III: ADVANCED TECHNIQUES - FINAL 1-3% BOOST
────────────────────────────────────────────────────────────────────────────────

ENSEMBLING
----------
Principle: Different models = different errors → average cancels errors out.
  - Diversity matters more than individual scores.
  - GBDT + NN better than GBDT + GBDT.

Methods:
  1. Simple blending: Weighted average (weights from CV scores).
  2. Stacking: Train meta-model on out-of-fold predictions from base models.
     (More complex, +0.5-1% over blending, requires time budget).

PSEUDO-LABELING
---------------
  - Use model predictions on test set as "labels" → retrain on train+test.
  - Only works if initial model is strong.
  - Must use CV framework to avoid leakage.

TEST-TIME AUGMENTATION (TTA)
-----------------------------
  - CV only: Create multiple augmented versions of test image → average predictions.
  - +0.5-1% boost, adds ~2x inference time.

RULE-BASED POST-PROCESSING
---------------------------
  - Apply domain heuristics to model outputs (e.g., "all instances of same name = same class").
  - High risk of overfitting to public LB - validate carefully.

────────────────────────────────────────────────────────────────────────────────
PART IV: COMMON PITFALLS - THESE CAUSE MOST FAILURES
────────────────────────────────────────────────────────────────────────────────

DATA LEAKAGE (CV=0.99, LB=0.50)
--------------------------------
  1. Target leakage: Features contain info not available at prediction time.
     - Check: ID columns, target-derived features, future information.
  2. Train-test contamination: Preprocessing on full dataset before split.
     - Fix: Fit scaler/normalizer ONLY on train fold, transform val fold.

OVERFITTING TO PUBLIC LEADERBOARD
----------------------------------
  - Public LB = small subset, often unrepresentative.
  - Solutions: Trust your CV, validate post-processing carefully, avoid excessive tuning to public LB.

MODEL TOO SLOW FOR TIME BUDGET
-------------------------------
  - EfficientNet-B4 + 3 folds + 10 epochs ≈ 50-60 min (exceeds 30-min budget).
  - Solution: Choose smaller model (B2/B3) or reduce folds/epochs BEFORE training.
  - Estimate time: (folds × epochs × min_per_epoch) before launch.

────────────────────────────────────────────────────────────────────────────────
QUICK DECISION TREE - WHAT TO USE
────────────────────────────────────────────────────────────────────────────────

START HERE:
  1. What's the domain?
     - Tabular       → LightGBM + feature engineering
     - Images        → EfficientNet-B2/B3 (30-min) or B4 (60-min) + MixUp/CutMix
     - Text          → DeBERTa fine-tuning
     - Time series   → Transform to tabular + LightGBM
     - Audio         → Mel-spectrogram + ResNet/EfficientNet

  2. What's the CV strategy?
     - Imbalanced    → StratifiedKFold
     - Grouped       → GroupKFold
     - Time-ordered  → TimeSeriesSplit
     - Default       → KFold (k=3 for speed, k=5 for robustness)

  3. What's the time budget?
     - 20-30 min     → 3 folds, smaller models (B2/B3), 6-8 epochs
     - 40-60 min     → 5 folds, larger models (B4/B5), 10 epochs
     - Estimate first: (folds × epochs × min_per_epoch) BEFORE training

  4. How to ensemble?
     - Time budget tight    → Simple weighted average (based on CV scores)
     - Time budget generous → Stacking (meta-model on OOF predictions)

────────────────────────────────────────────────────────────────────────────────

FINAL RULE: If CV score seems too good to be true, it is. Find the leak.

────────────────────────────────────────────────────────────────────────────────
