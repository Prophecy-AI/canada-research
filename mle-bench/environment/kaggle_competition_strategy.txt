The Kaggle Grandmaster's Playbook: A Synthesis of Winning Strategies and Architectures


________________


Part I: The Universal Kaggle Workflow


Success in competitive machine learning is predicated on a robust and disciplined workflow that transcends specific problem domains. The most effective participants build their strategies upon a foundation of rapid, iterative experimentation and meticulous, reliable validation. This section outlines these universal principles, which form the bedrock of a winning methodology.


Section 1: Foundational Principles for Competitive Machine Learning




1.1 Cultivating a Winning Mindset: Persistence, Iteration, and Community


Beyond technical skill, success in high-level competitions demands a specific mindset characterized by persistence and continuous learning. Progress is rarely linear; competitors often face performance plateaus that can only be overcome through sustained effort and iterative refinement.1 Breakthroughs are not typically the result of a single moment of inspiration but rather the cumulative effect of many small, incremental improvements.
A critical, and often underestimated, component of this process is active engagement with the Kaggle community. The platform's forums and public notebooks are invaluable resources. Analyzing past winning solutions and participating in discussions can provide game-changing insights that accelerate development.1 For instance, a competitor's discovery of the DeBERTa architecture's dominance in Natural Language Processing (NLP) competitions was sparked not by independent research, but by a forum comment from a Kaggle Grandmaster.3 This underscores the necessity of continuous learning by actively consuming relevant literature, blog posts, and community-generated content throughout a competition.2


1.2 The Engine of Success: Architecting a Fast and Flexible Experimentation Pipeline


The single most significant factor influencing success in a competition is the number of high-quality experiments that can be executed within the given timeframe.4 A high-velocity experimentation pipeline is, therefore, not a luxury but a core strategic asset. This requires optimizing the entire workflow for speed, from data preprocessing to model training and evaluation.
The practical implementation of such a pipeline is heavily reliant on specialized hardware and software. GPU acceleration is a key enabler, making computationally intensive techniques feasible within a competition's constraints. This involves leveraging GPU-accelerated libraries for both data manipulation (e.g., cuDF as a replacement for pandas) and model training (e.g., cuML, or the native GPU backends of XGBoost, LightGBM, and CatBoost).4 The strategic advantage conferred by this acceleration is profound; it creates a direct path to higher performance by expanding the searchable solution space. Advanced methods like multi-level stacking or iterative pseudo-labeling, which might take days to run on CPUs, can be completed in hours, transforming them from theoretical possibilities into practical tools.4 A robust pipeline is also a prerequisite for effectively leveraging community-sourced intelligence. While a forum post might reveal a superior but computationally demanding model like DeBERTa, only teams with an agile and powerful pipeline can overcome the associated "serious runtime issues" to capitalize on that public knowledge.3
The workflow should be structured for rapid iteration from the outset. A common best practice is to begin by building a diverse set of simple, fast baselines (e.g., logistic regression, a quick GBDT) to gain an initial understanding of the data's landscape before committing to a single, more complex modeling approach.4


Section 2: The Cornerstone of Reliability: Mastering Cross-Validation (CV)




2.1 Why Your Local CV is Your Most Important Metric


A frequent and critical error among competitors is overfitting to the public leaderboard. The public leaderboard is scored on a small, often non-representative subset of the test data, and a high rank on it provides a false sense of security. It is common for final private leaderboard rankings, calculated on a separate, larger dataset after the competition ends, to differ dramatically from the public leaderboard—an event known as a "shake-up".5
To avoid this pitfall, the most reliable measure of a model's generalization performance is a well-constructed local cross-validation (CV) scheme. The principle of "Trust your CV" is paramount.4 The primary goal is to design a validation strategy that accurately reflects the structure of the test data and the evaluation metric, providing an unbiased estimate of the final score.


2.2 Choosing the Right Strategy: A Guide to K-Fold, Stratified, Group, and Time-Series Splits


A simple, one-time split of data into training and validation sets is often insufficient and can be misleading. K-fold cross-validation is the industry standard, offering a more robust performance estimate by training and evaluating the model $k$ times on different subsets of the data, ensuring that every data point is used for both training and validation across the folds.4
Critically, the choice of CV strategy must be tailored to the underlying structure of the data to prevent data leakage and ensure the validation results are meaningful. The following table provides an actionable guide for selecting the appropriate CV strategy.


Problem/Data Structure
	Recommended CV Strategy
	Scikit-learn Class
	Key Consideration
	Standard Classification/Regression
	K-Fold
	KFold
	Good general-purpose default for homogenous data.
	Imbalanced Classification
	Stratified K-Fold
	StratifiedKFold
	Preserves the percentage of samples for each class in each fold.
	Grouped Data (e.g., by patient, user)
	Group K-Fold
	GroupKFold
	Ensures that observations from the same group do not appear in both training and testing sets within a split.4
	Time-Ordered Data
	Time Series Split
	TimeSeriesSplit
	Guarantees that the validation set is always chronologically after the training set, preventing the model from training on future data to predict the past.4
	________________


Part II: Domain-Specific Tactics and Architectures


While the principles of workflow and validation are universal, the choice of models and feature engineering techniques is highly domain-specific. This section details the dominant strategies and architectures for the most common types of Kaggle competitions.


Section 3: Conquering Tabular Competitions




3.1 The Unrivaled Power of Gradient Boosted Decision Trees (GBDTs)


For competitions involving structured, tabular data, Gradient Boosted Decision Trees (GBDTs) are the undisputed tool of choice.9 A survey of winning solutions reveals the consistent dominance of GBDT frameworks, particularly LightGBM, XGBoost, and CatBoost.9 Among these, LightGBM has emerged as a favorite among top competitors, often praised for its exceptional training speed and high performance on large datasets.9 GBDTs are particularly effective because they can model complex, non-linear interactions and are relatively insensitive to feature scaling, a common prerequisite for many neural network models.11 The following table compares the three leading GBDT libraries.
Model
	Key Strengths
	Common Use Cases/Considerations
	Performance Profile
	LightGBM
	Leaf-wise tree growth, histogram-based algorithm for speed.
	Excellent for large datasets where speed is critical. Can overfit on smaller datasets if not properly tuned.
	Very Fast
	XGBoost
	Level-wise tree growth, highly regularized, extensive parameter set for tuning.
	A robust and well-established choice. Can be slower than LightGBM but is often considered a very stable baseline.
	Fast
	CatBoost
	Symmetric trees to combat overfitting, built-in handling of categorical features.
	Ideal for datasets with many categorical variables, reducing the need for extensive preprocessing. Can be slower than LightGBM.
	Moderate
	

3.2 The Art of Tabular Feature Engineering


Despite the power of modern algorithms, the most significant differentiator in tabular competitions is often not the model, but the features it is trained on. Winning solutions consistently emphasize the importance of "heavy manual feature engineering".7 This demonstrates that automated tools have not yet supplanted the value of domain knowledge and creative data exploration. Success in this domain is as much a data analysis challenge as a modeling one. The process involves creating new, informative features from the existing data, such as:
* Interaction Features: Combining two or more features (e.g., feature_A * feature_B, feature_A / feature_B).
* Polynomial Features: Capturing non-linear relationships (e.g., $feature\_A^2$).
* Group-based Aggregations: Calculating statistics (e.g., mean, standard deviation, min, max) for numerical features, grouped by a categorical feature.
This process is highly iterative: new features are generated and then rigorously tested against the local CV setup to validate their contribution to model performance.4


3.3 Hybrid Approaches: Ensembling GBDTs with Neural Networks


While GBDTs form the core of most winning solutions, a clear trend among top-ranked teams is the use of hybrid ensembles that combine GBDTs with neural networks (NNs).9 This approach leverages a form of architectural symbiosis; the two model types capture different kinds of patterns in the data. GBDTs excel at modeling complex decision boundaries in heterogeneous feature spaces, while NNs, particularly architectures like Gated Recurrent Units (GRUs) or Transformers (e.g., TabNet), are better suited for learning from sequential data or discovering feature importance dynamically. The 1st place solution in the American Express default prediction competition, for example, used an ensemble of GBDTs and GRUs, with the latter specifically chosen to model the time-series aspects of customer statements.9 The final performance boost from adding an NN to a GBDT ensemble can be small—often around 0.5%—but is frequently the margin that separates top placements.9


Section 4: Excelling in Computer Vision (CV)




4.1 State-of-the-Art Architectures


In computer vision, the optimal architecture is highly dependent on the specific task. An analysis of winning solutions provides a clear map of the most effective and battle-tested models.
Task
	Dominant Architectures
	Common Backbones
	Classification
	EfficientNet (B0-B7), ResNeXt, Vision Transformer (ViT, BEiT, Dino)
	N/A
	Object Detection
	Faster R-CNN, YOLO, Cascade R-CNN
	ResNeXt, HRNet
	Segmentation
	U-Net, Feature Pyramid Network (FPN), DeepLabV3+
	EfficientNet, SE-ResNeXt, ResNet
	The consistent appearance of models from the ResNet family (ResNet, ResNeXt, SE-ResNeXt) and the EfficientNet family highlights their status as powerful and reliable backbones.12 More recently, Vision Transformers (ViT) and their derivatives (BEiT, Dino) have begun to feature prominently in winning solutions, representing a significant paradigm shift.13 The self-attention mechanism, which revolutionized NLP, is proving highly effective at modeling long-range dependencies across an image, often outperforming the more localized receptive fields of traditional Convolutional Neural Networks (CNNs). This signals a broader convergence of deep learning architectures, where fundamental concepts like attention are becoming increasingly domain-agnostic.


4.2 Essential Preprocessing and Data Augmentation


Effective data preparation is crucial in CV competitions. For specialized domains like medical imaging, which often use formats like DICOM, preprocessing is a multi-step process. This includes converting raw pixel data to standardized units like Hounsfield Units (HU) for CT scans, resampling images to an isotropic resolution to correct for variations between different scanners, and segmenting regions of interest (e.g., lungs) to focus the model's attention and reduce computational load.16 Techniques such as Contrast Limited Adaptive Histogram Equalization (CLAHE) are also commonly used to enhance image contrast, making subtle features more apparent.17
Data augmentation is an indispensable technique for improving model robustness and preventing overfitting.18 The Albumentations library is a popular and powerful tool for creating complex augmentation pipelines.20 Beyond standard geometric (flips, rotations) and color (brightness, contrast) transformations, advanced techniques are a hallmark of top solutions. MixUp and CutMix, for example, create new training samples by combining multiple images and their labels.13 These methods serve as a potent form of regularization, creating training examples that exist "in-between" distinct classes. This forces the model to learn smoother decision boundaries and prevents it from becoming overconfident, effectively teaching it about inter-class relationships rather than just class distinctions.22


Section 5: Dominating Natural Language Processing (NLP)




5.1 The Transformer Era: Leveraging BERT, RoBERTa, and DeBERTa


Modern NLP competitions are dominated by Transformer-based architectures.3 These models, pre-trained on vast text corpora, have established a new state-of-the-art. A clear evolution of performance is visible in winning solutions, progressing from the original BERT to RoBERTa (a more robustly trained version) and subsequently to DeBERTa, which introduced architectural innovations like disentangled attention and has been a consistent winner.3 The core strategy in nearly all NLP competitions is to fine-tune one of these large pre-trained models on the specific task dataset.25
This rapid evolution means that the competitive landscape in NLP is a "meta-game" of model selection. Top competitors gain an edge by being early adopters of new, superior architectures as they emerge from research. Success requires actively monitoring academic publications and community discussions to identify the next dominant model before it becomes widely adopted.3 This marks a fundamental shift from domains like tabular data, where feature engineering is paramount. In NLP, the focus has moved to "architectural engineering"—the art of selecting, fine-tuning, and combining the best pre-trained models, as the models themselves are responsible for learning the relevant features from text.


5.2 Advanced Fine-Tuning and Training Strategies


When the most powerful models are too large to meet competition constraints (e.g., inference time or memory limits), top teams employ advanced strategies. Knowledge distillation is one such technique, where a large, powerful "teacher" model is used to train a smaller, faster "student" model to replicate its predictions, effectively compressing its knowledge into a more efficient form.6 Pseudo-labeling, where a trained model generates labels for unlabeled data that is then used for retraining, is another powerful method for leveraging all available data.6
A common technical challenge in NLP is handling long sequences of text, as the computational complexity of standard Transformers scales quadratically with sequence length. Winning solutions address this by using specialized models like Longformer, or by implementing custom architectures that might, for example, add a Bi-directional Long Short-Term Memory (BiLSTM) layer on top of Transformer embeddings to better capture sequential information over long documents.26


Section 6: Strategies for Specialized Domains




6.1 Time Series Forecasting


The most effective strategy for time series forecasting competitions is to transform the problem into a tabular format and apply GBDTs.9 This transformation is achieved through extensive feature engineering, which typically includes:
* Time-based features: Extracting information from the timestamp, such as hour, day of the week, month, and holiday indicators.29
* Lag features: Using past values of the target series as input features to model serial dependencies.31
* Window features: Calculating rolling statistics (e.g., mean, median, standard deviation) over a defined time window.32
For validation, TimeSeriesSplit is essential to ensure that the model is always trained on past data to predict future data.8


6.2 Audio Classification


For audio tasks, the dominant paradigm is to convert the problem into an image classification task.12 This is done by converting the raw audio waveforms into 2D representations. Mel-spectrograms are the most widely used representation, as they map audio frequencies onto the mel scale, which approximates human auditory perception.12 Once the audio is represented as an image, standard 2D CNN architectures like ResNet or EfficientNet can be applied for classification.12 This "problem transformation" is a powerful meta-strategy, allowing competitors to leverage the highly mature and powerful toolsets developed for computer vision rather than creating domain-specific models from scratch.


6.3 Recommender Systems


Recommender systems are typically built on two main approaches: Collaborative Filtering, which uses the user-item interaction matrix to find similar users or items, and Content-Based Filtering, which uses item and user metadata.37 However, winning solutions almost invariably employ hybrid models that combine these techniques.39 This approach is a form of structured error correction; for example, content-based methods can provide reasonable recommendations for new users who have no interaction history (the "cold start" problem), a scenario where collaborative filtering fails.41 Deep learning is also increasingly used, with embedding layers learning dense vector representations of users and items to capture latent preferences.42
________________


Part III: Advanced Strategies for the Top Percentile


The techniques in this section often provide the marginal gains necessary to move from a good score to a winning one. They are typically more complex and computationally expensive, requiring a solid foundation to implement correctly.


Section 7: The Art of the Ensemble: Combining Models for Maximum Impact




7.1 The Principle of Diversity


Ensembling, or combining the predictions of multiple models, is a cornerstone of competitive machine learning. The underlying principle is that different models will make different errors, and by averaging their predictions, these errors can be cancelled out. The effectiveness of an ensemble is directly proportional to the diversity of the models it contains; the less correlated the models' errors, the greater the potential for improvement.44 Consequently, ensembling models with different architectures (e.g., a GBDT and a neural network) is generally more effective than ensembling models of the same type.4


7.2 Ensembling Techniques


Several methods are used to combine model predictions:
   * Simple Blending: This involves taking a simple or weighted average of the predictions from multiple models. Weights can be assigned based on the individual CV scores of each model.4
   * Hill Climbing: This is an automated search algorithm for finding optimal weights in a blend. It starts with the best-performing single model and iteratively adds other models to the ensemble, keeping a new model only if it improves the overall CV score.4
   * Stacking (Stacked Generalization): This is a more sophisticated, multi-level approach. First, a set of diverse "Level 0" base models is trained. Their out-of-fold (OOF) predictions on the training data are then used as input features to train a "Level 1" meta-model. This meta-model learns the optimal way to combine the base models' predictions, often outperforming simpler blending methods.4


Section 8: Squeezing Every Last Drop of Performance




8.1 Pseudo-Labeling: Turning Unlabeled Data into a Competitive Edge


Pseudo-labeling is a semi-supervised technique that leverages the unlabeled test set to improve model performance. A model is first trained on the available labeled data. This model is then used to make predictions on the test set. These predictions, or "pseudo-labels," are treated as ground truth and are added to the original training set. Finally, the model is retrained on this augmented dataset.4 For this technique to be effective, the initial model must be reasonably strong. It is also crucial to perform pseudo-labeling within a proper CV framework to avoid leaking information from the validation folds into their own training process.4


8.2 The Final Mile: Test-Time Augmentation (TTA) and Rule-Based Post-Processing


   * Test-Time Augmentation (TTA): In computer vision tasks, TTA is a technique used during inference to improve prediction robustness. Instead of making a prediction on a single test image, multiple augmented versions of the image (e.g., flipped, cropped, rotated) are created. The model makes a prediction for each augmented version, and these predictions are then aggregated (e.g., by averaging) to produce a final, more stable prediction.12
   * Rule-Based Post-Processing: This involves applying domain-specific heuristics to model outputs to correct for known systematic errors or to enforce constraints that the model cannot learn. For example, in a text competition, a rule might enforce that if one instance of a name is identified as a NAME_STUDENT, all other instances of that same name in the document should also be classified as such.12 This step is highly competition-specific and can provide a significant boost on the leaderboard, but it also carries a high risk of overfitting to quirks in the training or public test data if not validated carefully.45


Section 9: Avoiding Catastrophic Pitfalls: A Primer on Data Leakage


Data leakage is one of the most severe problems in applied machine learning, leading to models that appear highly accurate during development but fail completely when deployed. An expert competitor must develop a mindset of deep skepticism, especially when a model's performance seems "too good to be true".47 The first reaction to an unexpectedly high CV score should not be celebration, but a rigorous investigation to find the source of the likely leak.


9.1 Identifying and Preventing Target Leakage


Target leakage occurs when the training data contains features that would not be available at the time of prediction in a real-world scenario.47 This creates a spurious relationship that the model exploits, leading to inflated performance metrics. A classic example is predicting a medical diagnosis using a feature representing a treatment that is only administered after the diagnosis is made.48 Detecting this requires careful, chronological thinking about the data generation process.


9.2 The Dangers of Train-Test Contamination


This form of leakage happens when information from the validation or test set inadvertently influences the training process. A common cause is performing data preprocessing steps, such as fitting a scaler or an imputer, on the entire dataset before splitting it into training and validation sets. The correct procedure is to fit any such transformers only on the training data and then use the fitted objects to transform the validation and test sets. This ensures that no information about the distribution of the validation data influences the training process.47


Conclusion


An analysis of winning Kaggle solutions reveals a clear and consistent playbook for success. It is a multi-layered strategy that begins not with a specific algorithm, but with a disciplined and efficient workflow built around rapid experimentation and rigorous, appropriate cross-validation. While state-of-the-art architectures are domain-specific—GBDTs for tabular data, Transformers for NLP, and CNNs/ViTs for computer vision—their successful application universally depends on meticulous data preprocessing, thoughtful feature engineering, and robust augmentation. The final margin of victory is often secured through advanced techniques like stacking, pseudo-labeling, and carefully validated post-processing. Ultimately, the path to the top of the leaderboard is a synthesis of foundational discipline, domain-specific expertise, and the creative application of advanced methods, all while maintaining a vigilant awareness of potential pitfalls like data leakage.
Works cited
   1. 7 Essential Tips to Become a Successful Kaggle Competition Master | by Lucien Lin, accessed October 16, 2025, https://medium.com/@lucien1999s.pro/7-essential-tips-to-become-a-successful-kaggle-competition-master-c2e2f36dddba
   2. Tips and Tricks to succeed on Kaggle | Kaggle, accessed October 16, 2025, https://www.kaggle.com/general/269183
   3. Sanjid Hasan | Kaggle, accessed October 16, 2025, https://www.kaggle.com/sanjidh090
   4. The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling ..., accessed October 16, 2025, https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/
   5. Natural Language Processing with Disaster Tweets | Kaggle, accessed October 16, 2025, https://www.kaggle.com/competitions/nlp-getting-started
   6. Essay: Competition Cards & Recent NLP competitions - Kaggle, accessed October 16, 2025, https://www.kaggle.com/code/kashnitsky/essay-competition-cards-recent-nlp-competitions
   7. Winning Tips on Machine Learning Competitions by Kazanova ..., accessed October 16, 2025, https://www.hackerearth.com/practice/machine-learning/advanced-techniques/winning-tips-machine-learning-competitions-kazanova-current-kaggle-3/tutorial/
   8. TimeSeriesSplit — scikit-learn 1.7.2 documentation, accessed October 16, 2025, https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html
   9. Tabular Data Competitions - Winning Strategies - ML Contests, accessed October 16, 2025, https://mlcontests.com/tabular-data/
   10. CatBoost v. XGBoost v. LightGBM - Kaggle, accessed October 16, 2025, https://www.kaggle.com/code/nholloway/catboost-v-xgboost-v-lightgbm
   11. A Guide to any Classification Problem - Kaggle, accessed October 16, 2025, https://www.kaggle.com/code/durgancegaur/a-guide-to-any-classification-problem
   12. Kaggle Past Solutions, accessed October 16, 2025, https://ndres.me/kaggle-past-solutions/
   13. 30th place solution (vision transformer) | Kaggle, accessed October 16, 2025, https://www.kaggle.com/competitions/cmi-detect-behavior-with-sensor-data/writeups/30th-place-solution-vision-transformer
   14. 1st Place - Winning Solution - Placeholder | Kaggle, accessed October 16, 2025, https://www.kaggle.com/competitions/petfinder-pawpularity-score/writeups/giba-rapids-svr-magic-1st-place-winning-solution-p
   15. RSNA Screening Mammography Breast Cancer Detection | Kaggle, accessed October 16, 2025, https://www.kaggle.com/competitions/rsna-breast-cancer-detection/discussion/372567
   16. Full Preprocessing Tutorial - Kaggle, accessed October 16, 2025, https://www.kaggle.com/code/gzuidhof/full-preprocessing-tutorial
   17. Clahe Preprocessed Medical Imaging Dataset - Kaggle, accessed October 16, 2025, https://www.kaggle.com/datasets/heartzhacker/n-clahe
   18. Documentation - Albumentations, accessed October 16, 2025, https://albumentations.ai/docs/1-introduction/what-are-image-augmentations/
   19. Getting Started with Image Preprocessing in Python - Kaggle, accessed October 16, 2025, https://www.kaggle.com/code/rimmelasghar/getting-started-with-image-preprocessing-in-python
   20. Data Augmentation Tutorial: Basic, Cutout, Mixup - Kaggle, accessed October 16, 2025, https://www.kaggle.com/code/kaushal2896/data-augmentation-tutorial-basic-cutout-mixup
   21. mixup, cutmix, albumentation - Kaggle, accessed October 16, 2025, https://www.kaggle.com/code/prajeshsanghvi/mixup-cutmix-albumentation
   22. Deep CNN Model with Mixup and Cutmix Augmentation - Kaggle, accessed October 16, 2025, https://www.kaggle.com/code/shawon10/deep-cnn-model-with-mixup-and-cutmix-augmentation
   23. Data Augmentation for Object Detection - Kaggle, accessed October 16, 2025, https://www.kaggle.com/code/ankursingh12/data-augmentation-for-object-detection
   24. 10 Leading Language Models For NLP In 2022 - TOPBOTS, accessed October 16, 2025, https://www.topbots.com/leading-nlp-language-models-2020/
   25. Text Classification Techniques - Explained - Kaggle, accessed October 16, 2025, https://www.kaggle.com/code/eraikako/text-classification-techniques-explained
   26. 1st place solution - Ensemble of diverse Deberta architectures and ..., accessed October 16, 2025, https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/writeups/fold-zero-1st-place-solution-ensemble-of-diverse-d
   27. Why Attention Is All You Need? | Kaggle, accessed October 16, 2025, https://www.kaggle.com/general/493003
   28. 1st Place Solution | Kaggle, accessed October 16, 2025, https://www.kaggle.com/competitions/rohlik-sales-forecasting-challenge-v2/discussion/563215
   29. Time Series Analysis With Feature Engineering - Kaggle, accessed October 16, 2025, https://www.kaggle.com/code/queensaikia/time-series-analysis-with-feature-engineering
   30. Time Series II || Feature Engineering Concepts - Kaggle, accessed October 16, 2025, https://www.kaggle.com/code/janiobachmann/time-series-ii-feature-engineering-concepts
   31. Time Series as Features - Kaggle, accessed October 16, 2025, https://www.kaggle.com/code/ryanholbrook/time-series-as-features
   32. Feature Engineering for Time Series - Kaggle, accessed October 16, 2025, https://www.kaggle.com/code/patrickurbanke/feature-engineering-for-time-series
   33. CNN 2D Basic Solution Powered by fast.ai - Kaggle, accessed October 16, 2025, https://www.kaggle.com/code/jtv199/cnn-2d-basic-solution-powered-by-fast-ai
   34. Classify MNIST Audio using Spectrograms/Keras CNN - Kaggle, accessed October 16, 2025, https://www.kaggle.com/code/christianlillelund/classify-mnist-audio-using-spectrograms-keras-cnn
   35. ESC-50: CNN for Spectrogram Classification - Kaggle, accessed October 16, 2025, https://www.kaggle.com/code/salimhammadi07/esc-50-cnn-for-spectrogram-classification
   36. crlandsc/Music-Genre-Classification-Using-Convolutional ... - GitHub, accessed October 16, 2025, https://github.com/crlandsc/Music-Genre-Classification-Using-Convolutional-Neural-Networks
   37. Recommender Systems in Python 101 - Kaggle, accessed October 16, 2025, https://www.kaggle.com/code/gspmoreira/recommender-systems-in-python-101
   38. Hybrid Recommender System - Kaggle, accessed October 16, 2025, https://www.kaggle.com/code/iambideniz/hybrid-recommender-system
   39. Hybrid Movie Recommender System - Kaggle, accessed October 16, 2025, https://www.kaggle.com/code/mustafaoz158/hybrid-movie-recommender-system
   40. Hybrid RecSys Evaluation - Kaggle, accessed October 16, 2025, https://www.kaggle.com/code/gpreda/hybrid-recsys-evaluation
   41. A Guide to Collaborative Recommenders - Kaggle, accessed October 16, 2025, https://www.kaggle.com/code/furth3r/a-guide-to-collaborative-recommenders
   42. Recommender System Deep Learning - Kaggle, accessed October 16, 2025, https://www.kaggle.com/code/taruntiwarihp/recommender-system-deep-learning
   43. Deep Learning based Recommender Systems - Kaggle, accessed October 16, 2025, https://www.kaggle.com/code/jamesloy/deep-learning-based-recommender-systems
   44. Ensemble Models for Classification - Kaggle, accessed October 16, 2025, https://www.kaggle.com/code/zahrazolghadr/ensemble-models-for-classification
   45. Winning solutions of kaggle competitions, accessed October 16, 2025, https://www.kaggle.com/code/sudalairajkumar/winning-solutions-of-kaggle-competitions/notebook?scriptVersionId=4534558
   46. Winning solutions of kaggle competitions, accessed October 16, 2025, https://www.kaggle.com/code/sudalairajkumar/winning-solutions-of-kaggle-competitions
   47. Data Leakage - Kaggle, accessed October 16, 2025, https://www.kaggle.com/code/dansbecker/data-leakage
   48. Data Leakage - Kaggle, accessed October 16, 2025, https://www.kaggle.com/code/alexisbcook/data-leakage